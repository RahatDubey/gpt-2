{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=1.13.0\t\t   LICENSE\t\t\t       test2.npz\r\n",
      "accumulate.py\t   load_dataset.py\t\t       test2.txt\r\n",
      "better.txt\t   memory_saving_gradients.py\t       test.npz\r\n",
      "Chatbotter2.ipynb  model.py\t\t\t       test.txt\r\n",
      "Chatbotter.ipynb   models\t\t\t       toastme2.npz\r\n",
      "checkpoint\t   My First Project-01f0de489aed.json  ToastMeDF.ipynb\r\n",
      "CONTRIBUTORS.md    prog.ipynb\t\t\t       toastme.npz\r\n",
      "DEVELOPERS.md\t   __pycache__\t\t\t       toastme.txt\r\n",
      "Dockerfile.cpu\t   README.md\t\t\t       train-horovod.py\r\n",
      "Dockerfile.gpu\t   requirements.txt\t\t       train.py\r\n",
      "download_model.py  sample.py\t\t\t       Untitled1.ipynb\r\n",
      "encode.py\t   samples\t\t\t       Untitled.ipynb\r\n",
      "encoder.py\t   src\r\n"
     ]
    }
   ],
   "source": [
    "!ls\n",
    "# import sys\n",
    "# reload(sys)\n",
    "# sys.setdefaultencoding(\"utf-8\") THIS IS BAD PRACTICE BUT IT WORKS FOR JUPYTER NOTEBOOKS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting fire>=0.1.3\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d9/69/faeaae8687f4de0f5973694d02e9d6c3eb827636a009157352d98de1129e/fire-0.2.1.tar.gz (76kB)\n",
      "\u001b[K     |████████████████████████████████| 81kB 10.0MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting regex==2017.4.5\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/36/62/c0c0d762ffd4ffaf39f372eb8561b8d491a11ace5a7884610424a8b40f95/regex-2017.04.05.tar.gz (601kB)\n",
      "\u001b[K     |████████████████████████████████| 604kB 23.9MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting requests==2.21.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/e3/20f3d364d6c8e5d2353c72a67778eb189176f08e873c9900e10c0287b84b/requests-2.21.0-py2.py3-none-any.whl (57kB)\n",
      "\u001b[K     |████████████████████████████████| 61kB 1.2MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting tqdm==4.31.1\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6c/4b/c38b5144cf167c4f52288517436ccafefe9dc01b8d1c190e18a6b154cd4a/tqdm-4.31.1-py2.py3-none-any.whl (48kB)\n",
      "\u001b[K     |████████████████████████████████| 51kB 14.1MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting toposort==1.5\n",
      "  Downloading https://files.pythonhosted.org/packages/e9/8a/321cd8ea5f4a22a06e3ba30ef31ec33bea11a3443eeb1d89807640ee6ed4/toposort-1.5-py2.py3-none-any.whl\n",
      "Requirement already satisfied: six in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from fire>=0.1.3->-r requirements.txt (line 1)) (1.11.0)\n",
      "Requirement already satisfied: termcolor in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from fire>=0.1.3->-r requirements.txt (line 1)) (1.1.0)\n",
      "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from requests==2.21.0->-r requirements.txt (line 3)) (1.23)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from requests==2.21.0->-r requirements.txt (line 3)) (2019.9.11)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from requests==2.21.0->-r requirements.txt (line 3)) (2.6)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from requests==2.21.0->-r requirements.txt (line 3)) (3.0.4)\n",
      "Building wheels for collected packages: fire, regex\n",
      "  Building wheel for fire (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for fire: filename=fire-0.2.1-py2.py3-none-any.whl size=103543 sha256=047efcaed4c4c318280078325c6e824665caed562018eeb134338b0030aea98d\n",
      "  Stored in directory: /home/ec2-user/.cache/pip/wheels/31/9c/c0/07b6dc7faf1844bb4688f46b569efe6cafaa2179c95db821da\n",
      "  Building wheel for regex (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for regex: filename=regex-2017.4.5-cp36-cp36m-linux_x86_64.whl size=564371 sha256=7d67d54b7cce3929cc719ff131eb7e45d51784e4520c0828e429d4917e8fdfb8\n",
      "  Stored in directory: /home/ec2-user/.cache/pip/wheels/75/07/38/3c16b529d50cb4e0cd3dbc7b75cece8a09c132692c74450b01\n",
      "Successfully built fire regex\n",
      "Installing collected packages: fire, regex, requests, tqdm, toposort\n",
      "  Found existing installation: requests 2.20.0\n",
      "    Uninstalling requests-2.20.0:\n",
      "      Successfully uninstalled requests-2.20.0\n",
      "Successfully installed fire-0.2.1 regex-2017.4.5 requests-2.21.0 toposort-1.5 tqdm-4.31.1\n",
      "\u001b[33mWARNING: You are using pip version 19.3.1; however, version 20.0.2 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip3 install -r requirements.txt #use docker.gpu or venv if on local "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching checkpoint: 1.00kit [00:00, 1.22Mit/s]                                                     \n",
      "Fetching encoder.json: 1.04Mit [00:00, 53.1Mit/s]                                                   \n",
      "Fetching hparams.json: 1.00kit [00:00, 1.60Mit/s]                                                   \n",
      "Fetching model.ckpt.data-00000-of-00001: 498Mit [00:05, 90.8Mit/s]                                  \n",
      "Fetching model.ckpt.index: 6.00kit [00:00, 6.85Mit/s]                                               \n",
      "Fetching model.ckpt.meta: 472kit [00:00, 43.4Mit/s]                                                 \n",
      "Fetching vocab.bpe: 457kit [00:00, 46.2Mit/s]                                                       \n"
     ]
    }
   ],
   "source": [
    "!python download_model.py 117M #go into encode.py and change default to 345 ALSO TRAIN.PY change to 345m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/home/ec2-user/SageMaker/gpt-2/src\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "!export PYTHONIOENCODING=UTF-8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/SageMaker/gpt-2\r\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_core/__init__.py:1467: The name tf.estimator.inputs is deprecated. Please use tf.compat.v1.estimator.inputs instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ec2-user/SageMaker/gpt-2/model.py:147: The name tf.AUTO_REUSE is deprecated. Please use tf.compat.v1.AUTO_REUSE instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import fire\n",
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import model, sample, encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#import generate_unconditional_samples\n",
    "import interactive_conditional_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_core/__init__.py:1467: The name tf.estimator.inputs is deprecated. Please use tf.compat.v1.estimator.inputs instead.\n",
      "\n",
      "Reading files\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:12<00:00, 12.85s/it]\n",
      "Writing test2.npz\n"
     ]
    }
   ],
   "source": [
    "!python encode.py test2.txt test2.npz\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learning rate default=0.00002\n",
    "#default batch_size should be one\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_core/__init__.py:1467: The name tf.estimator.inputs is deprecated. Please use tf.compat.v1.estimator.inputs instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ec2-user/SageMaker/gpt-2/model.py:147: The name tf.AUTO_REUSE is deprecated. Please use tf.compat.v1.AUTO_REUSE instead.\n",
      "\n",
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From /home/ec2-user/SageMaker/gpt-2/memory_saving_gradients.py:13: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.\n",
      "\n",
      "WARNING:tensorflow:From train.py:88: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From train.py:91: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "2020-03-03 15:38:14.772993: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations:  AVX512F\n",
      "To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2020-03-03 15:38:14.796996: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2999995000 Hz\n",
      "2020-03-03 15:38:14.798575: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55fffd36ac70 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2020-03-03 15:38:14.798592: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2020-03-03 15:38:14.800372: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n",
      "WARNING:tensorflow:From train.py:92: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ec2-user/SageMaker/gpt-2/model.py:148: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ec2-user/SageMaker/gpt-2/model.py:152: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ec2-user/SageMaker/gpt-2/model.py:36: The name tf.rsqrt is deprecated. Please use tf.math.rsqrt instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ec2-user/SageMaker/gpt-2/model.py:166: The name tf.add_to_collection is deprecated. Please use tf.compat.v1.add_to_collection instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ec2-user/SageMaker/gpt-2/sample.py:65: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.cast` instead.\n",
      "WARNING:tensorflow:From /home/ec2-user/SageMaker/gpt-2/sample.py:16: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /home/ec2-user/SageMaker/gpt-2/sample.py:70: multinomial (from tensorflow.python.ops.random_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.random.categorical` instead.\n",
      "WARNING:tensorflow:From train.py:117: The name tf.trainable_variables is deprecated. Please use tf.compat.v1.trainable_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From train.py:121: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From train.py:144: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.\n",
      "\n",
      "WARNING:tensorflow:From train.py:147: The name tf.summary.merge is deprecated. Please use tf.compat.v1.summary.merge instead.\n",
      "\n",
      "WARNING:tensorflow:From train.py:149: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.\n",
      "\n",
      "WARNING:tensorflow:From train.py:152: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n",
      "\n",
      "WARNING:tensorflow:From train.py:156: The name tf.global_variables_initializer is deprecated. Please use tf.compat.v1.global_variables_initializer instead.\n",
      "\n",
      "Loading checkpoint models/117M/model.ckpt\n",
      "Loading dataset...\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 12.56it/s]\n",
      "dataset has 2816957 tokens\n",
      "Training...\n",
      "[1 | 18.88] loss=3.69 avg=3.69\n",
      "[2 | 21.64] loss=4.63 avg=4.16\n",
      "[3 | 24.38] loss=4.13 avg=4.15\n",
      "[4 | 27.14] loss=3.83 avg=4.07\n",
      "[5 | 29.88] loss=3.78 avg=4.01\n",
      "[6 | 32.63] loss=3.98 avg=4.00\n",
      "[7 | 35.39] loss=3.86 avg=3.98\n",
      "[8 | 38.33] loss=3.66 avg=3.94\n",
      "[9 | 41.23] loss=3.56 avg=3.90\n",
      "[10 | 43.97] loss=3.67 avg=3.87\n",
      "[11 | 46.71] loss=3.58 avg=3.85\n",
      "[12 | 49.44] loss=3.57 avg=3.82\n",
      "[13 | 52.19] loss=3.47 avg=3.79\n",
      "[14 | 54.94] loss=3.47 avg=3.77\n",
      "[15 | 57.68] loss=3.21 avg=3.73\n",
      "[16 | 60.42] loss=3.29 avg=3.70\n",
      "[17 | 63.15] loss=3.47 avg=3.69\n",
      "[18 | 65.91] loss=3.19 avg=3.65\n",
      "[19 | 68.67] loss=3.33 avg=3.64\n",
      "[20 | 71.42] loss=3.49 avg=3.63\n",
      "[21 | 74.17] loss=3.13 avg=3.60\n",
      "[22 | 76.90] loss=3.30 avg=3.59\n",
      "[23 | 79.64] loss=3.20 avg=3.57\n",
      "[24 | 82.39] loss=3.20 avg=3.55\n",
      "[25 | 85.13] loss=3.32 avg=3.54\n",
      "[26 | 87.89] loss=3.41 avg=3.53\n",
      "[27 | 90.63] loss=3.20 avg=3.52\n",
      "[28 | 93.37] loss=3.44 avg=3.52\n",
      "[29 | 96.12] loss=3.04 avg=3.50\n",
      "[30 | 98.85] loss=3.26 avg=3.49\n",
      "[31 | 101.60] loss=3.14 avg=3.48\n",
      "[32 | 104.35] loss=3.27 avg=3.47\n",
      "[33 | 107.10] loss=3.33 avg=3.46\n",
      "[34 | 109.85] loss=3.41 avg=3.46\n",
      "[35 | 112.59] loss=3.21 avg=3.45\n",
      "[36 | 115.33] loss=3.09 avg=3.44\n",
      "[37 | 118.07] loss=3.31 avg=3.44\n",
      "[38 | 120.81] loss=3.17 avg=3.43\n",
      "[39 | 123.55] loss=3.40 avg=3.43\n",
      "[40 | 126.31] loss=3.33 avg=3.42\n",
      "[41 | 129.06] loss=3.09 avg=3.41\n",
      "[42 | 131.80] loss=3.33 avg=3.41\n",
      "[43 | 134.54] loss=3.18 avg=3.41\n",
      "[44 | 137.30] loss=3.09 avg=3.40\n",
      "[45 | 140.05] loss=3.31 avg=3.39\n",
      "[46 | 142.80] loss=2.87 avg=3.38\n",
      "[47 | 145.55] loss=2.94 avg=3.37\n",
      "[48 | 148.30] loss=3.11 avg=3.36\n",
      "[49 | 151.04] loss=3.01 avg=3.35\n",
      "[50 | 153.79] loss=3.21 avg=3.35\n",
      "[51 | 156.54] loss=2.93 avg=3.34\n",
      "[52 | 159.28] loss=3.06 avg=3.33\n",
      "[53 | 162.03] loss=3.14 avg=3.33\n",
      "[54 | 164.77] loss=3.15 avg=3.32\n",
      "[55 | 167.52] loss=3.17 avg=3.32\n",
      "[56 | 170.26] loss=3.00 avg=3.31\n",
      "[57 | 173.01] loss=3.11 avg=3.31\n",
      "[58 | 175.75] loss=3.37 avg=3.31\n",
      "[59 | 178.49] loss=3.10 avg=3.30\n",
      "[60 | 181.24] loss=2.80 avg=3.29\n",
      "[61 | 183.98] loss=2.98 avg=3.29\n",
      "[62 | 186.72] loss=2.97 avg=3.28\n",
      "[63 | 189.46] loss=3.06 avg=3.27\n",
      "[64 | 192.21] loss=3.17 avg=3.27\n",
      "[65 | 194.97] loss=3.17 avg=3.27\n",
      "[66 | 197.72] loss=5.26 avg=3.31\n",
      "[67 | 200.47] loss=3.11 avg=3.31\n",
      "[68 | 203.22] loss=2.88 avg=3.30\n",
      "[69 | 205.97] loss=3.15 avg=3.30\n",
      "[70 | 208.73] loss=3.30 avg=3.30\n",
      "[71 | 211.48] loss=2.92 avg=3.29\n",
      "[72 | 214.23] loss=3.06 avg=3.28\n",
      "[73 | 217.14] loss=2.98 avg=3.28\n",
      "[74 | 220.09] loss=3.11 avg=3.28\n",
      "[75 | 222.84] loss=3.38 avg=3.28\n",
      "[76 | 225.57] loss=2.75 avg=3.27\n",
      "[77 | 228.32] loss=3.11 avg=3.26\n",
      "[78 | 231.06] loss=2.87 avg=3.26\n",
      "[79 | 233.81] loss=3.23 avg=3.26\n",
      "[80 | 236.53] loss=3.10 avg=3.25\n",
      "[81 | 239.26] loss=3.10 avg=3.25\n",
      "[82 | 242.01] loss=2.97 avg=3.25\n",
      "[83 | 244.75] loss=2.67 avg=3.24\n",
      "[84 | 247.48] loss=3.17 avg=3.23\n",
      "[85 | 250.23] loss=3.09 avg=3.23\n",
      "[86 | 252.96] loss=3.14 avg=3.23\n",
      "[87 | 255.71] loss=2.99 avg=3.23\n",
      "[88 | 258.46] loss=3.20 avg=3.23\n",
      "[89 | 261.20] loss=2.84 avg=3.22\n",
      "[90 | 263.94] loss=2.75 avg=3.21\n",
      "[91 | 266.68] loss=2.86 avg=3.21\n",
      "[92 | 269.43] loss=2.79 avg=3.20\n",
      "[93 | 272.17] loss=3.38 avg=3.20\n",
      "[94 | 274.90] loss=2.74 avg=3.19\n",
      "[95 | 277.63] loss=3.20 avg=3.19\n",
      "[96 | 280.37] loss=3.18 avg=3.19\n",
      "[97 | 283.10] loss=2.90 avg=3.19\n",
      "[98 | 285.84] loss=2.88 avg=3.18\n",
      "[99 | 288.58] loss=3.11 avg=3.18\n",
      "Generating samples...\n",
      "======== SAMPLE 1 ========\n",
      " I a bunch of fun.  Good luck, I'm sure you’re going to be a wonderful fellow.\" \"There’s nothing better about an internet stranger than telling you to “go your own way�⣁.����‍‍‍‍‣�‍‍‍“‣�E?\"\n",
      "\"Got picked on a lot recently and it seems like every girl likes my body. Could definitely use some positivity.\" \"I’m always in the dark on my phone, my brain getting a little lost, and my brain telling me when I’ll not want to be alone.\"\n",
      "\"I love my hair and skin and I just want to toast me!\" \"Hey! I’m a fan of your hair!\"\n",
      "\"I have a little toasty beard and could really use a pick me up. :)\" \"You're not one-dimensional. The other ones are just amazing! The rest of the world, I know, is pretty similar but I've put them in the past few months. All the other hair models out there seem like more efficient people.\"\n",
      "\"This is the hardest birthday i have ever had, so i guess it’s not gonna pass\" \"Happy Birthday!\"\n",
      "\"19y and no makeup. Been feeling insecure about my looks because I don’t look like a boy but today I am one. I just want to enjoy this life more much!\" \"You look really beautiful to me. Also, I was only two years old and I had pretty big breasts for a while but that was it. \n",
      "\n",
      "I can’t believe i am just sitting back and not smiling - I have no sense of fashion and my body does not match the beauty I’ve seen in several other photos. And with a little effort I can imagine how beautiful I’ve come at the age of 13.\n",
      "\n",
      "You are beautiful and it’s such a huge accomplishment that is accomplished.\n",
      "\n",
      "I love your teeth, lips and hair and I am so proud of you. Keep pushing, and I know your nose is an awesome smile. And a smile that suits me.\"\n",
      "\"I have no sense of style and my skin doesn’t hold up; sorry I'm not doing my duty, but I can't get off this. I'm in a depression episode and just feeling really ugly right now, please toast me.\" \"Your lips, and skin are amazing on you! Maybe you can see what someone else says to you, too. \"\n",
      "\"Life is rough around here!\" \"Been feeling a little down here all day.\"\n",
      "\"In my life, i'm not good at what I see, which is why i want to escape. I’m just so sorry I don't recognize myself. Please toast me.\" \"Your face, eyes, and smile.  The best things you have to offer us are amazing!\"\n",
      "\"Going through a rough patch with the biggest breakup in 5 hours. Don’t know if my life will be alright.\" \"i think i'm pretty cute, and i'd appreciate a hug :)\"\n",
      "\"(R/toastme, im a boy for this one)\" \"You're a cute chap.. I know you seem to be extremely handsome! \"\n",
      "\"I'm so lonesome I can't take this anymore. A week away from my usual family and friends and not for long from the front door.... I just need some positive reinforcement....\" \"I'm sorry. I'm just so lonesome I can take this anymore. A week away from my usual family and friends and not for long from the front door.... I just need some positive reinforcement....\"\n",
      "\"Please do my best as a boy. Being in the early 30s for my first date after college is hard because I'm mentally challenged and have low self esteem for sure.  \n",
      "\n",
      "i also feel a lot younger than I really are, so I'll be honest to say I do seem a bit older but we're not done for sure. \n",
      "\n",
      "i don't give a fuck what you think im doing because I have low self esteem for sure but hey its gonna get better i promise. (You have your best wishes to )\"\n",
      "\"M23, still in the middle of it but trying to lose weight. I'm in this shit hole of doubt, and this summer is finally starting to make me feel better... I guess. Toast me?\" \"You look so good. \n",
      "\n",
      "But the last few months have really helped me my depression is pretty much gone and I am feeling healthier and I could do with some motivation to try new things.\"\n",
      "\"Got rejected for a job in the last few hours because I hate my face. Don't feel happy and my self esteem is at an all time low. Please toast me.\" \"Lovely job! You're cute, and honestly you're a brilliant work ethic fan\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100 | 346.06] loss=2.77 avg=3.18\n",
      "[101 | 348.79] loss=3.10 avg=3.18\n",
      "[102 | 351.54] loss=2.67 avg=3.17\n",
      "[103 | 354.27] loss=2.93 avg=3.16\n",
      "[104 | 357.03] loss=2.90 avg=3.16\n",
      "[105 | 359.77] loss=2.81 avg=3.15\n",
      "[106 | 362.52] loss=2.67 avg=3.15\n",
      "[107 | 365.26] loss=2.78 avg=3.14\n",
      "[108 | 368.00] loss=3.06 avg=3.14\n",
      "[109 | 370.75] loss=2.87 avg=3.14\n",
      "[110 | 373.49] loss=2.96 avg=3.13\n",
      "[111 | 376.23] loss=2.80 avg=3.13\n",
      "[112 | 378.97] loss=2.96 avg=3.13\n",
      "[113 | 381.71] loss=3.02 avg=3.12\n",
      "[114 | 384.46] loss=2.81 avg=3.12\n",
      "[115 | 387.22] loss=3.06 avg=3.12\n",
      "[116 | 389.98] loss=3.04 avg=3.12\n",
      "[117 | 392.73] loss=2.80 avg=3.11\n",
      "[118 | 395.50] loss=3.03 avg=3.11\n",
      "[119 | 398.45] loss=3.16 avg=3.11\n",
      "[120 | 401.37] loss=2.84 avg=3.11\n",
      "[121 | 404.12] loss=3.16 avg=3.11\n",
      "[122 | 406.86] loss=2.58 avg=3.10\n",
      "[123 | 409.60] loss=3.28 avg=3.10\n",
      "[124 | 412.33] loss=2.74 avg=3.10\n",
      "[125 | 415.08] loss=2.93 avg=3.10\n",
      "[126 | 417.83] loss=2.71 avg=3.09\n",
      "[127 | 420.57] loss=3.28 avg=3.09\n",
      "[128 | 423.32] loss=3.02 avg=3.09\n",
      "[129 | 426.07] loss=2.71 avg=3.09\n",
      "[130 | 428.82] loss=2.85 avg=3.08\n",
      "[131 | 431.55] loss=2.44 avg=3.08\n",
      "[132 | 434.29] loss=2.88 avg=3.07\n",
      "[133 | 437.03] loss=2.86 avg=3.07\n",
      "[134 | 439.77] loss=2.57 avg=3.06\n",
      "[135 | 442.52] loss=2.74 avg=3.06\n",
      "[136 | 445.26] loss=2.77 avg=3.06\n",
      "[137 | 448.00] loss=2.84 avg=3.05\n",
      "[138 | 450.76] loss=2.74 avg=3.05\n",
      "[139 | 453.51] loss=2.92 avg=3.05\n",
      "[140 | 456.26] loss=2.67 avg=3.04\n",
      "[141 | 459.02] loss=2.88 avg=3.04\n",
      "[142 | 461.76] loss=2.91 avg=3.04\n",
      "[143 | 464.50] loss=2.83 avg=3.04\n",
      "[144 | 467.24] loss=2.80 avg=3.03\n",
      "[145 | 469.99] loss=2.62 avg=3.03\n",
      "[146 | 472.74] loss=2.84 avg=3.02\n",
      "[147 | 475.51] loss=2.93 avg=3.02\n",
      "[148 | 478.25] loss=2.82 avg=3.02\n",
      "[149 | 481.00] loss=3.02 avg=3.02\n",
      "[150 | 483.73] loss=2.48 avg=3.01\n",
      "[151 | 486.47] loss=2.37 avg=3.01\n",
      "[152 | 489.21] loss=3.01 avg=3.01\n",
      "[153 | 491.95] loss=3.18 avg=3.01\n",
      "[154 | 494.71] loss=2.96 avg=3.01\n",
      "[155 | 497.44] loss=2.41 avg=3.00\n",
      "[156 | 500.19] loss=2.46 avg=2.99\n",
      "[157 | 502.95] loss=2.79 avg=2.99\n",
      "[158 | 505.69] loss=2.99 avg=2.99\n",
      "[159 | 508.44] loss=2.69 avg=2.99\n",
      "[160 | 511.19] loss=2.86 avg=2.98\n",
      "[161 | 513.94] loss=2.83 avg=2.98\n",
      "[162 | 516.67] loss=3.00 avg=2.98\n",
      "[163 | 519.42] loss=2.85 avg=2.98\n",
      "[164 | 522.17] loss=2.88 avg=2.98\n",
      "[165 | 524.89] loss=2.95 avg=2.98\n",
      "[166 | 527.62] loss=2.91 avg=2.98\n",
      "[167 | 530.36] loss=2.56 avg=2.97\n",
      "[168 | 533.11] loss=3.01 avg=2.97\n",
      "[169 | 535.88] loss=2.79 avg=2.97\n",
      "[170 | 538.62] loss=2.67 avg=2.97\n",
      "[171 | 541.36] loss=2.78 avg=2.97\n",
      "[172 | 544.11] loss=2.92 avg=2.97\n",
      "[173 | 546.83] loss=2.22 avg=2.96\n",
      "[174 | 549.57] loss=2.83 avg=2.95\n",
      "[175 | 552.33] loss=2.73 avg=2.95\n",
      "[176 | 555.07] loss=2.65 avg=2.95\n",
      "[177 | 557.82] loss=2.94 avg=2.95\n",
      "[178 | 560.54] loss=3.14 avg=2.95\n",
      "[179 | 563.28] loss=2.72 avg=2.95\n",
      "[180 | 566.03] loss=2.48 avg=2.94\n",
      "[181 | 568.78] loss=2.65 avg=2.94\n",
      "[182 | 571.54] loss=2.57 avg=2.93\n",
      "[183 | 574.29] loss=2.82 avg=2.93\n",
      "[184 | 577.21] loss=2.74 avg=2.93\n",
      "[185 | 580.14] loss=2.22 avg=2.92\n",
      "[186 | 582.88] loss=2.77 avg=2.92\n",
      "[187 | 585.65] loss=2.90 avg=2.92\n",
      "[188 | 588.38] loss=3.00 avg=2.92\n",
      "[189 | 591.11] loss=2.73 avg=2.92\n",
      "[190 | 593.85] loss=2.83 avg=2.92\n",
      "[191 | 596.60] loss=2.84 avg=2.92\n",
      "[192 | 599.34] loss=2.73 avg=2.91\n",
      "[193 | 602.10] loss=2.73 avg=2.91\n",
      "[194 | 604.86] loss=2.44 avg=2.91\n",
      "[195 | 607.59] loss=2.96 avg=2.91\n",
      "[196 | 610.35] loss=2.67 avg=2.90\n",
      "[197 | 613.09] loss=2.74 avg=2.90\n",
      "[198 | 615.82] loss=2.68 avg=2.90\n",
      "[199 | 618.57] loss=2.71 avg=2.90\n",
      "Generating samples...\n",
      "======== SAMPLE 1 ========\n",
      " an\n",
      "\n",
      "���������������� I��������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������\n",
      "\n",
      "[200 | 668.62] loss=2.42 avg=2.89\n",
      "[201 | 671.38] loss=2.18 avg=2.88\n",
      "[202 | 674.13] loss=2.87 avg=2.88\n",
      "[203 | 676.87] loss=2.59 avg=2.88\n",
      "[204 | 679.62] loss=2.64 avg=2.88\n",
      "[205 | 682.38] loss=2.39 avg=2.87\n",
      "[206 | 685.12] loss=2.70 avg=2.87\n",
      "[207 | 687.85] loss=2.66 avg=2.87\n",
      "[208 | 690.61] loss=2.65 avg=2.87\n",
      "[209 | 693.35] loss=2.80 avg=2.86\n",
      "[210 | 696.12] loss=2.69 avg=2.86\n",
      "[211 | 698.87] loss=2.99 avg=2.86\n",
      "[212 | 701.60] loss=2.57 avg=2.86\n",
      "[213 | 704.35] loss=2.67 avg=2.86\n",
      "[214 | 707.08] loss=2.63 avg=2.86\n",
      "[215 | 709.82] loss=2.85 avg=2.86\n",
      "[216 | 712.56] loss=2.64 avg=2.85\n",
      "[217 | 715.30] loss=2.84 avg=2.85\n",
      "[218 | 718.06] loss=2.54 avg=2.85\n",
      "[219 | 720.81] loss=2.77 avg=2.85\n",
      "[220 | 723.55] loss=2.74 avg=2.85\n",
      "[221 | 726.29] loss=2.56 avg=2.84\n",
      "[222 | 729.03] loss=2.55 avg=2.84\n",
      "[223 | 731.80] loss=2.50 avg=2.84\n",
      "[224 | 734.53] loss=2.76 avg=2.84\n",
      "[225 | 737.28] loss=2.84 avg=2.84\n",
      "[226 | 740.03] loss=2.95 avg=2.84\n",
      "[227 | 742.79] loss=2.56 avg=2.83\n",
      "[228 | 745.52] loss=2.59 avg=2.83\n",
      "[229 | 748.26] loss=3.08 avg=2.83\n",
      "[230 | 751.00] loss=2.38 avg=2.83\n",
      "[231 | 753.74] loss=2.87 avg=2.83\n",
      "[232 | 756.57] loss=2.64 avg=2.83\n",
      "[233 | 759.54] loss=2.43 avg=2.82\n",
      "[234 | 762.33] loss=2.57 avg=2.82\n",
      "[235 | 765.06] loss=2.84 avg=2.82\n",
      "[236 | 767.81] loss=2.62 avg=2.82\n",
      "[237 | 770.57] loss=2.90 avg=2.82\n",
      "[238 | 773.32] loss=2.75 avg=2.82\n",
      "[239 | 776.08] loss=2.26 avg=2.81\n",
      "[240 | 778.84] loss=2.66 avg=2.81\n",
      "[241 | 781.60] loss=2.72 avg=2.81\n",
      "[242 | 784.36] loss=2.73 avg=2.81\n",
      "[243 | 787.09] loss=2.94 avg=2.81\n",
      "[244 | 789.85] loss=2.14 avg=2.80\n",
      "[245 | 792.61] loss=2.59 avg=2.80\n",
      "[246 | 795.34] loss=2.87 avg=2.80\n",
      "[247 | 798.11] loss=2.81 avg=2.80\n",
      "[248 | 800.85] loss=2.73 avg=2.80\n",
      "[249 | 803.60] loss=2.07 avg=2.79\n",
      "[250 | 806.36] loss=2.99 avg=2.80\n",
      "[251 | 809.13] loss=2.69 avg=2.79\n",
      "[252 | 811.89] loss=2.49 avg=2.79\n",
      "[253 | 814.65] loss=2.34 avg=2.79\n",
      "[254 | 817.40] loss=2.79 avg=2.79\n",
      "[255 | 820.15] loss=2.55 avg=2.78\n",
      "[256 | 822.89] loss=2.33 avg=2.78\n",
      "[257 | 825.63] loss=2.90 avg=2.78\n",
      "[258 | 828.37] loss=2.73 avg=2.78\n",
      "[259 | 831.11] loss=2.78 avg=2.78\n",
      "[260 | 833.85] loss=2.77 avg=2.78\n",
      "[261 | 836.59] loss=2.67 avg=2.78\n",
      "[262 | 839.33] loss=2.88 avg=2.78\n",
      "[263 | 842.06] loss=2.51 avg=2.78\n",
      "[264 | 844.81] loss=2.60 avg=2.77\n",
      "[265 | 847.54] loss=2.79 avg=2.77\n",
      "[266 | 850.29] loss=2.97 avg=2.78\n",
      "[267 | 853.04] loss=2.78 avg=2.78\n",
      "[268 | 855.78] loss=3.12 avg=2.78\n",
      "[269 | 858.53] loss=2.62 avg=2.78\n",
      "[270 | 861.27] loss=2.81 avg=2.78\n",
      "[271 | 864.02] loss=2.51 avg=2.78\n",
      "[272 | 866.76] loss=2.67 avg=2.78\n",
      "[273 | 869.51] loss=2.32 avg=2.77\n",
      "[274 | 872.25] loss=3.41 avg=2.78\n",
      "[275 | 875.01] loss=2.59 avg=2.78\n",
      "[276 | 877.76] loss=2.46 avg=2.77\n",
      "[277 | 880.51] loss=2.36 avg=2.77\n",
      "[278 | 883.27] loss=2.13 avg=2.76\n",
      "[279 | 886.01] loss=2.69 avg=2.76\n",
      "[280 | 888.78] loss=2.59 avg=2.76\n",
      "[281 | 891.52] loss=2.52 avg=2.76\n",
      "[282 | 894.28] loss=2.66 avg=2.75\n",
      "[283 | 897.02] loss=2.82 avg=2.76\n",
      "[284 | 899.77] loss=2.59 avg=2.75\n",
      "[285 | 902.51] loss=2.92 avg=2.76\n",
      "[286 | 905.25] loss=2.82 avg=2.76\n",
      "[287 | 908.00] loss=2.26 avg=2.75\n",
      "[288 | 910.76] loss=2.42 avg=2.75\n",
      "[289 | 913.53] loss=2.51 avg=2.74\n",
      "[290 | 916.27] loss=2.51 avg=2.74\n",
      "[291 | 919.02] loss=2.65 avg=2.74\n",
      "[292 | 921.77] loss=2.08 avg=2.73\n",
      "[293 | 924.52] loss=2.55 avg=2.73\n",
      "[294 | 927.26] loss=2.32 avg=2.73\n",
      "[295 | 930.02] loss=2.62 avg=2.73\n",
      "[296 | 932.76] loss=2.54 avg=2.72\n",
      "[297 | 935.53] loss=2.68 avg=2.72\n",
      "[298 | 938.47] loss=2.75 avg=2.72\n",
      "[299 | 941.38] loss=2.62 avg=2.72\n",
      "Generating samples...\n",
      "======== SAMPLE 1 ========\n",
      " a friend since 7 and finding out she lost the chance. Just looking for hope, and always hoping for the best.\"\n",
      "\"M25. Just got rejected by the girl I liked a lot.\" \"Wow those lips and the eyes are on point!  \n",
      "\n",
      "The kind words are also so beautiful on the other side of your eyes I wish life had turned out so differently as well. Love it or hate it but you are clearly beautiful and you're lovely and I can see that in your eyes. I want someone really great and good to love you and I know it's not easy but it is just as hard to believe in someone, try and be genuine but you're very beautiful. I love how easy you are to get upset with. Good luck man and take a deep breath! :)\n",
      "\n",
      "&amp;#x200B;\n",
      "\n",
      "You remind me of the guy from my high school days. I got called a 6-year old before getting the full attention to a big guy. I wanted to see if I could just go with his look or my looks. I had that feeling but I got a little too lost in it all. Maybe I could try a new thing and see if I could just do it myself? I think it's pretty simple as fuck. So with my experiences with that feeling and being around and seeing things through. You need to have a confidence that people don't have. Just try not to hate yourself too much too much and just make the steps to find something that you don't like. You'll get there. You'll get there.\"\n",
      "\"Today was my bday. It was my anniversary. It was my anniversary of how I can now feel like yesterday. I feel amazing today.\" \"I could do a toast to the day I was born, but today is my bday. It is like I have zero days on the birthday of my birth even when I start out in the other side of the street\"\n",
      "\"I have a terrible head about this, i feel like my mother is the most she's ever had and my brother is an awful sociopath. I could really use a toast right now.\" \"Congratulations! You should be proud. That’s such an incredible head! Beautiful eyes and gorgeous face!\n",
      "I’m sure that’ll get better. You’re gorgeous though. Keep going!\n",
      "\n",
      "Ps- I’ve also had a bad head of hair from being 12-13 with my mum. Not to mention my mother has been practically inseparable from me. I’m just now starting to feel as though I’m not really any different than my sister. Just need to be there for myself (i am not).\n",
      "\n",
      "Ps- you’re beautiful and i’m so jealous of you even had a hand full of friends she was never close with so you’re so much better than if i had known you but now i’ve never met all of you in my childhood. I truly love it. I love everyone.\n",
      "\n",
      "Ps- I have a horrible migraine and have been having trouble with accepting that I was so happy for so long. I can’t even take a picture of me without feeling broken. I feel as if no one ever truly cared about me even once they saw me smiling.\" \"You look like a very chill and fun person which is really going to happen to you, as you don’t even have any control over yourself or things. You’re gonna do a lot.\n",
      "\n",
      "I have a bit of criticism for people in my past. I could really do with a compliment to make me want to say something like yours if someone cared.\n",
      "\n",
      "It’s ok to be bad ?\"\n",
      "\"[27M] Just got dumped after 4+ months with horrible self esteem issues. Looking back there’s nothing I can say I can say I saw how many people I could have guessed as it was my first experience with feeling worthless and unlovable. I didn’t even know I felt worthless because I had the world's most beautiful eyes. I just saw one thing that had me believing about that world.\" \"You're cute as a button! Great features!  My boyfriend hates it! There is love in here! Be happy with what you see? Keep on feeling!\"\n",
      "\"Been really down for a month and im in a pretty dark place. Could use a toast right now\" \"I like your hair and skin! It suits you so much.\"\n",
      "\"I got out of an abusive marriage even though I just fell out with a girl I loved. Help would be so much appreciated. (he/him)\" \"Hey man, life can get so hard sometimes, but sometimes it’s about being a little bit happy because you have that nice smile, that lovely smile, and you don’t have to be alone, because you have a lot going for you in your life.\n",
      "\n",
      "I�\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[300 | 991.52] loss=2.23 avg=2.72\n",
      "[301 | 994.27] loss=2.34 avg=2.71\n",
      "[302 | 997.01] loss=2.77 avg=2.71\n",
      "[303 | 999.77] loss=2.66 avg=2.71\n",
      "[304 | 1002.50] loss=2.61 avg=2.71\n",
      "[305 | 1005.24] loss=2.18 avg=2.71\n",
      "[306 | 1007.98] loss=2.72 avg=2.71\n",
      "[307 | 1010.73] loss=2.45 avg=2.71\n",
      "[308 | 1013.48] loss=2.94 avg=2.71\n",
      "[309 | 1016.23] loss=2.39 avg=2.70\n",
      "[310 | 1018.96] loss=2.70 avg=2.70\n",
      "[311 | 1021.71] loss=2.52 avg=2.70\n",
      "[312 | 1024.47] loss=2.54 avg=2.70\n",
      "[313 | 1027.20] loss=2.40 avg=2.70\n",
      "[314 | 1029.97] loss=2.22 avg=2.69\n",
      "[315 | 1032.70] loss=2.48 avg=2.69\n",
      "[316 | 1035.45] loss=2.87 avg=2.69\n",
      "[317 | 1038.20] loss=2.61 avg=2.69\n",
      "[318 | 1040.94] loss=2.64 avg=2.69\n",
      "[319 | 1043.69] loss=2.44 avg=2.69\n",
      "[320 | 1046.44] loss=2.39 avg=2.68\n",
      "[321 | 1049.19] loss=3.07 avg=2.69\n",
      "[322 | 1051.93] loss=1.99 avg=2.68\n",
      "[323 | 1054.69] loss=6.23 avg=2.72\n",
      "[324 | 1057.44] loss=2.61 avg=2.72\n",
      "[325 | 1060.18] loss=2.46 avg=2.71\n",
      "[326 | 1062.93] loss=2.76 avg=2.72\n",
      "[327 | 1065.66] loss=2.26 avg=2.71\n",
      "[328 | 1068.40] loss=2.65 avg=2.71\n",
      "[329 | 1071.15] loss=2.26 avg=2.71\n",
      "[330 | 1073.90] loss=2.83 avg=2.71\n",
      "[331 | 1076.64] loss=2.72 avg=2.71\n",
      "[332 | 1079.41] loss=2.92 avg=2.71\n",
      "[333 | 1082.15] loss=2.15 avg=2.70\n",
      "[334 | 1084.88] loss=2.18 avg=2.70\n",
      "[335 | 1087.63] loss=2.54 avg=2.70\n",
      "[336 | 1090.38] loss=2.26 avg=2.69\n",
      "[337 | 1093.12] loss=2.23 avg=2.69\n",
      "[338 | 1095.88] loss=2.29 avg=2.68\n",
      "[339 | 1098.62] loss=2.17 avg=2.68\n",
      "[340 | 1101.36] loss=2.34 avg=2.67\n",
      "[341 | 1104.13] loss=2.58 avg=2.67\n",
      "[342 | 1106.88] loss=3.02 avg=2.68\n",
      "[343 | 1109.64] loss=2.75 avg=2.68\n",
      "[344 | 1112.39] loss=2.90 avg=2.68\n",
      "[345 | 1115.15] loss=2.57 avg=2.68\n",
      "[346 | 1118.14] loss=2.03 avg=2.67\n",
      "[347 | 1121.02] loss=2.74 avg=2.67\n",
      "[348 | 1123.78] loss=2.75 avg=2.67\n",
      "[349 | 1126.51] loss=2.94 avg=2.68\n",
      "[350 | 1129.26] loss=2.28 avg=2.67\n",
      "[351 | 1131.98] loss=2.36 avg=2.67\n",
      "[352 | 1134.75] loss=2.44 avg=2.67\n",
      "[353 | 1137.50] loss=2.75 avg=2.67\n",
      "[354 | 1140.24] loss=2.84 avg=2.67\n",
      "[355 | 1142.97] loss=3.45 avg=2.68\n",
      "[356 | 1145.73] loss=2.32 avg=2.67\n",
      "[357 | 1148.46] loss=2.49 avg=2.67\n",
      "[358 | 1151.21] loss=2.81 avg=2.67\n",
      "[359 | 1153.96] loss=2.49 avg=2.67\n",
      "[360 | 1156.71] loss=2.89 avg=2.67\n",
      "[361 | 1159.47] loss=2.08 avg=2.67\n",
      "[362 | 1162.22] loss=2.54 avg=2.67\n",
      "[363 | 1164.97] loss=2.69 avg=2.67\n",
      "[364 | 1167.72] loss=3.10 avg=2.67\n",
      "[365 | 1170.48] loss=2.10 avg=2.66\n",
      "[366 | 1173.23] loss=2.73 avg=2.67\n",
      "[367 | 1176.00] loss=2.57 avg=2.66\n",
      "[368 | 1178.76] loss=2.38 avg=2.66\n",
      "[369 | 1181.50] loss=3.06 avg=2.67\n",
      "[370 | 1184.26] loss=2.96 avg=2.67\n",
      "[371 | 1186.99] loss=2.61 avg=2.67\n",
      "[372 | 1189.75] loss=2.51 avg=2.67\n",
      "[373 | 1192.49] loss=2.63 avg=2.67\n",
      "[374 | 1195.24] loss=3.08 avg=2.67\n",
      "[375 | 1197.99] loss=3.07 avg=2.67\n",
      "[376 | 1200.74] loss=2.16 avg=2.67\n",
      "[377 | 1203.50] loss=3.03 avg=2.67\n",
      "[378 | 1206.25] loss=2.89 avg=2.67\n",
      "[379 | 1209.01] loss=2.66 avg=2.67\n",
      "[380 | 1211.75] loss=2.37 avg=2.67\n",
      "[381 | 1214.49] loss=2.03 avg=2.66\n",
      "[382 | 1217.25] loss=2.68 avg=2.67\n",
      "[383 | 1219.98] loss=2.41 avg=2.66\n",
      "[384 | 1222.74] loss=2.31 avg=2.66\n",
      "[385 | 1225.50] loss=2.65 avg=2.66\n",
      "[386 | 1228.24] loss=2.65 avg=2.66\n",
      "[387 | 1230.97] loss=2.39 avg=2.66\n",
      "[388 | 1233.71] loss=2.85 avg=2.66\n",
      "[389 | 1236.46] loss=2.47 avg=2.66\n",
      "[390 | 1239.21] loss=2.60 avg=2.66\n",
      "[391 | 1241.95] loss=2.74 avg=2.66\n",
      "[392 | 1244.69] loss=2.70 avg=2.66\n",
      "[393 | 1247.44] loss=2.77 avg=2.66\n",
      "[394 | 1250.19] loss=2.29 avg=2.65\n",
      "[395 | 1252.92] loss=2.66 avg=2.65\n",
      "[396 | 1255.66] loss=2.63 avg=2.65\n",
      "[397 | 1258.41] loss=2.45 avg=2.65\n",
      "[398 | 1261.16] loss=2.57 avg=2.65\n",
      "[399 | 1263.91] loss=2.22 avg=2.65\n",
      "Generating samples...\n",
      "======== SAMPLE 1 ========\n",
      " normal and not enough to make it to where you are tonight! There are more people who are willing to talk about your age and personality than those without looking at you. It’s not just because you have a big smile, it is because you want to feel good and confident in your career. I know how it sounds and I’m just saying it so I’m not a bit retarded\" \"Dude, I mean that, youre beautiful. You are very pretty, you have your beautiful glasses! Also, that looks like you’re in a very interesting conversation to be sitting down in with you, so good luck :)❤️\"\n",
      "\"Lost my job and haven’t started life in years, could use some positivity.\" \"You have a really nice haircut, I wish you the best and I'm really sorry that won't happen. I'm going to miss you too, but you are so pretty and I hope you're still finding the strength, as I have been saying with all this. If you ever ever can, please send me a message if it's really a need, if you ever want to make an early-post-chat chat or just for anyone who wants to chat, I could definitely help.\"\n",
      "\"22F, socially awkward at college with no friends, no one likes or talks to me, no one really cares, and I am just generally feeling like I’m not good enough\" \"I think the way it's been said that it sounds like it's been a shitty week. You're pretty good with yourself.\"\n",
      "\"I overdosed on Klonopin after being drugged at a party. I stayed in psychiatric hospitals where I was diagnosed with Borderline Personality. My boyfriend of four years ghosted me for this for a year and proceeded to break up with me over the phone while I was in the hospital. I really need a win.\" \"Your smile is infectious. I would say that your smile is worth it. I mean, that’s one of the best smiles I can muster. I’d like to compliment you on that smile that.\"\n",
      "\"I’m recovering from surgery and started cardiac rehab again! It’s been rough so far, but I’m so proud of myself since I lost my job and started to get back on my feet!\" \"You have very nice eyes and your hair is beautiful! We have all of you. Love. \"\n",
      "\"It’s my birthday [29] and I’m spending it alone, and in pain. Wouldn’t refuse a little love if you have any to spare.\" \"You are so absolutely gorgeous! I also like how you look like you are dressed.\"\n",
      "\"13M, recently diagnosed bipolar, and dealing with some stuff right now. Kinda scared of what is coming up for me. Need some encouragement :(\" \"#I dont need to tell you that you’re so hot lol\"\n",
      "\"I need a bit of a pick-me-up\" \"Dude, I kinda look like your username. It's like a drawing where everything is in place and everyone keeps doing it for every thing, like painting, playing, playing, anything, whatever happens.\"\n",
      "\"Hi reddit. I am Prince and all that has happened is that I look the same. Please tell me something I don’t understand (please dont delete it) about my appearance\" \"You look gorgeous.\"\n",
      "\"20yr old guy. This has been going on for years. It’s frustrating and I’m not doing well at college. I feel like no one knows how old I am. I’m just feeling really lonely and lacking any direction.\" \"If it were me who should take responsibility in college, I'd think there's nobody smarter than myself. Plus, I think this is one of the smartest people in the world. Also, you're the cutest person who I can see on Reddit. I mean, you have gorgeous eyes, nice skin and a nice smile, I love you even if you don't know the cuteness. There's nobody better than you to do the right thing you can do. I really like your cuteness, I totally respect that!\"\n",
      "\"Hey, reddit. I feel like no one knows how old I am. I just feel kinda alone and tired. Would like some kind words before I actually start looking at myself to make me think I'm ugly.\" \"I really hope things get better. Forgive me for the pain. I would like for a toast.\"\n",
      "\"22F My grandmother passed away suddenly and completely in pain, and I’m in my pyjamas because I don't do basic chores. Every waking moment I struggle to remember my mother's presence. Toastme ♥️\" \"You are strong in courage to stand up for this.\"\n",
      "\"[23M] Never been in a relationship and struggling to make it work. Put on\n",
      "\n",
      "[400 | 1315.26] loss=2.15 avg=2.64\n",
      "[401 | 1318.02] loss=2.77 avg=2.64\n",
      "[402 | 1320.77] loss=2.52 avg=2.64\n",
      "[403 | 1323.54] loss=2.44 avg=2.64\n",
      "[404 | 1326.27] loss=2.74 avg=2.64\n",
      "[405 | 1329.01] loss=2.66 avg=2.64\n",
      "[406 | 1331.77] loss=2.85 avg=2.64\n",
      "[407 | 1334.51] loss=2.30 avg=2.64\n",
      "[408 | 1337.25] loss=2.67 avg=2.64\n",
      "[409 | 1339.99] loss=2.60 avg=2.64\n",
      "[410 | 1342.73] loss=2.53 avg=2.64\n",
      "[411 | 1345.46] loss=3.12 avg=2.64\n",
      "[412 | 1348.21] loss=2.44 avg=2.64\n",
      "[413 | 1350.95] loss=2.57 avg=2.64\n",
      "[414 | 1353.70] loss=2.69 avg=2.64\n",
      "[415 | 1356.45] loss=3.36 avg=2.65\n",
      "[416 | 1359.19] loss=2.44 avg=2.65\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[417 | 1361.94] loss=2.75 avg=2.65\n",
      "[418 | 1364.68] loss=2.45 avg=2.65\n",
      "[419 | 1367.42] loss=2.76 avg=2.65\n",
      "[420 | 1370.15] loss=2.57 avg=2.65\n",
      "[421 | 1372.89] loss=2.39 avg=2.64\n",
      "[422 | 1375.63] loss=2.12 avg=2.64\n",
      "[423 | 1378.38] loss=2.12 avg=2.63\n",
      "[424 | 1381.12] loss=2.61 avg=2.63\n",
      "[425 | 1383.86] loss=2.38 avg=2.63\n",
      "[426 | 1386.62] loss=2.49 avg=2.63\n",
      "[427 | 1389.37] loss=2.20 avg=2.62\n",
      "[428 | 1392.14] loss=2.43 avg=2.62\n",
      "[429 | 1394.87] loss=2.41 avg=2.62\n",
      "[430 | 1397.62] loss=2.52 avg=2.62\n",
      "[431 | 1400.36] loss=2.61 avg=2.62\n",
      "[432 | 1403.12] loss=2.51 avg=2.62\n",
      "[433 | 1405.86] loss=2.67 avg=2.62\n",
      "[434 | 1408.61] loss=2.38 avg=2.62\n",
      "[435 | 1411.35] loss=2.65 avg=2.62\n",
      "[436 | 1414.11] loss=3.03 avg=2.62\n",
      "[437 | 1416.86] loss=2.39 avg=2.62\n",
      "[438 | 1419.61] loss=2.63 avg=2.62\n",
      "[439 | 1422.36] loss=2.27 avg=2.61\n",
      "[440 | 1425.10] loss=2.73 avg=2.62\n",
      "[441 | 1427.83] loss=2.69 avg=2.62\n",
      "[442 | 1430.57] loss=2.36 avg=2.61\n",
      "[443 | 1433.32] loss=2.69 avg=2.61\n",
      "[444 | 1436.08] loss=2.78 avg=2.62\n",
      "[445 | 1438.82] loss=2.85 avg=2.62\n",
      "[446 | 1441.56] loss=2.15 avg=2.61\n",
      "[447 | 1444.31] loss=2.14 avg=2.61\n",
      "[448 | 1447.05] loss=2.04 avg=2.60\n",
      "[449 | 1449.82] loss=2.26 avg=2.60\n",
      "[450 | 1452.56] loss=2.79 avg=2.60\n",
      "[451 | 1455.30] loss=2.72 avg=2.60\n",
      "[452 | 1458.05] loss=2.17 avg=2.60\n",
      "[453 | 1460.80] loss=2.81 avg=2.60\n",
      "[454 | 1463.53] loss=2.45 avg=2.60\n",
      "[455 | 1466.28] loss=2.28 avg=2.60\n",
      "[456 | 1469.03] loss=2.76 avg=2.60\n",
      "[457 | 1471.78] loss=2.41 avg=2.60\n",
      "[458 | 1474.52] loss=2.27 avg=2.59\n",
      "[459 | 1477.44] loss=2.29 avg=2.59\n",
      "[460 | 1480.39] loss=2.26 avg=2.59\n",
      "[461 | 1483.14] loss=3.06 avg=2.59\n",
      "[462 | 1485.89] loss=2.69 avg=2.59\n",
      "[463 | 1488.64] loss=2.57 avg=2.59\n",
      "[464 | 1491.38] loss=2.54 avg=2.59\n",
      "[465 | 1494.15] loss=2.18 avg=2.59\n",
      "[466 | 1496.90] loss=2.53 avg=2.59\n",
      "[467 | 1499.64] loss=2.36 avg=2.58\n",
      "[468 | 1502.39] loss=2.55 avg=2.58\n",
      "[469 | 1505.12] loss=2.79 avg=2.59\n",
      "[470 | 1507.87] loss=2.36 avg=2.58\n",
      "[471 | 1510.62] loss=2.17 avg=2.58\n",
      "[472 | 1513.35] loss=2.24 avg=2.58\n",
      "[473 | 1516.10] loss=2.60 avg=2.58\n",
      "[474 | 1518.85] loss=2.48 avg=2.58\n",
      "[475 | 1521.59] loss=2.30 avg=2.57\n",
      "[476 | 1524.35] loss=2.35 avg=2.57\n",
      "[477 | 1527.09] loss=2.50 avg=2.57\n",
      "[478 | 1529.85] loss=2.70 avg=2.57\n",
      "[479 | 1532.59] loss=3.73 avg=2.58\n",
      "[480 | 1535.33] loss=2.87 avg=2.59\n",
      "[481 | 1538.07] loss=2.94 avg=2.59\n",
      "[482 | 1540.83] loss=2.41 avg=2.59\n",
      "[483 | 1543.59] loss=2.86 avg=2.59\n",
      "[484 | 1546.34] loss=2.38 avg=2.59\n",
      "[485 | 1549.10] loss=2.56 avg=2.59\n",
      "[486 | 1551.85] loss=2.87 avg=2.59\n",
      "[487 | 1554.59] loss=2.86 avg=2.59\n",
      "[488 | 1557.35] loss=2.26 avg=2.59\n",
      "[489 | 1560.09] loss=2.28 avg=2.59\n",
      "[490 | 1562.83] loss=2.88 avg=2.59\n",
      "[491 | 1565.59] loss=2.36 avg=2.59\n",
      "[492 | 1568.32] loss=2.66 avg=2.59\n",
      "[493 | 1571.08] loss=2.53 avg=2.59\n",
      "[494 | 1573.82] loss=2.76 avg=2.59\n",
      "[495 | 1576.57] loss=2.51 avg=2.59\n",
      "[496 | 1579.30] loss=2.85 avg=2.59\n",
      "[497 | 1582.04] loss=2.52 avg=2.59\n",
      "[498 | 1584.80] loss=2.35 avg=2.59\n",
      "[499 | 1587.55] loss=2.54 avg=2.59\n",
      "Generating samples...\n",
      "======== SAMPLE 1 ========\n",
      "Hey all. Just wanted to know what your hobbies are like.\n",
      "\n",
      "I am a guy with autism who has been going through a rough time with depression, anxiety, and ED. For whatever reason I am still alive. But yesterday was my 13th birthday, and I'm spending the rest with family. This year was my 16th birthday and I'm spending it with family. This is my 13th birthday and I'm spending it alone. So I'm kinda feeling pretty low today. Would appreciate any kindness.\" \"There is no doubt about that!\n",
      "\n",
      "\"Dealing with a very recent and difficult breakup/enlargement of my nose and cheeks. Not a big fan of my nose/face, but I've always been a big fan of the shape of my nose and cheeks! I'm sorry about my nose/beard combo and I’m hoping my nose looks good for the first time in October!\" \"I think your nose/beard combo fits your nose nicely!\"\n",
      "\"My boyfriend broke up with me today. I’m in the middle of the worst depression episode I’ve ever had and that’s when the most important part of my life goes awry and in search of anything positive. I could use some good vibes. 1.5 years ago this was the worst thing ever happen to me and my boyfriend. But my girlfriend just told me she was too lost and won’t be able to do anything. So please just say something, do the rest of your best to help yourself keep going. If nothing else I would be. I could really use positivity.\" \"Girl, I’m the only one who’s ever willing to be with someone who is just so so full of love.\"\n",
      "\"19F. First time ever seeing someone attractive. But I just wanna be honest: I didn't really notice until now. So yeah, you look really pretty in the picture, and your hair really suits you really well. I hope you have an awesome day!\" \"I’m really happy for you for going after that. You look gorgeous. I’m sure you will be blessed in her future. Take a good toast and get out there and help this lady that doesn't make it through the day! Xoxoxoxous! xx\"\n",
      "\"13yo girl here. Roast Me?! I need some encouragement. Please be kind to me\" \"You have great eyes and your hair is amazing ���� ����\"\n",
      "\"41 both parents got sick with Cancer and Dad has been dead since October. No real joy in life. Feel worthless and unattractive and old. Everything feels meaningless. Toast me please?\" \"You should be sad. Get a job with someone else! Don't care. You got this.\"\n",
      "\"I’m starting my physical transition from ftm, and I’m struggling with low self-esteem because I’m so used to being fetishized as an as an asian woman but I feel completely unattractive due to my appearance\" \"Holy shit! Your hair is amazing!\"\n",
      "\"After 16 years of being fetishized as an asian man, I'm starting to feel much more comfortable as an asian man. I already have a positive outlook on everything. I'm starting to feel a bit like I’m healthy and happier. However, I have a very low self-esteem and feel like all my friends hate me because I'm too skinny for my face. Some encouragement would be greatly appreciated.\" \"Being a vegetarian is pretty amazing stuff for a young lady. You look awesome.\"\n",
      "\"Turned 21 yesterday and already had an uneven eating disorder. I thought I was healthy but my jaw keeps throwing up against everything. I'm in the middle of an eating disorder relapse so here goes: me and my cat, bf of 10 years left me on Tuesday, give me your best idea of what's going on.\" \"You look awesome and you look like an amazing guy. I wish that you had your cats as well, please, don't beat yourself up for wanting a speedy recovery. You deserve so much more!\"\n",
      "\"I'm having a hard time liking what people say about me. Some toasts would be very nice!\" \"You look kind and beautiful, and you look nice, keep your head up strong! I have no doubt in your eyes that you will make the world seem better and better! Hang in there! \"\n",
      "\"After 8 years of being in the closeted I'm still used to being positive but today was the worst year of my life. I broke the negative perceptions of myself and the people who took me down. Please toast me!\" \"Good luck!\"\n",
      "\"It's been 4 days since Friday, but on Tuesday I realized I couldn't hold anything but a handful. I'm tired, ugly, and you guys are amazing! Toast me!\" \"You got this!\"\n",
      "\"(M17) Recently fell victim to alcoholism. I've been dealing with chronic illness (\n",
      "\n",
      "[500 | 1637.67] loss=2.20 avg=2.58\n",
      "[501 | 1640.42] loss=2.68 avg=2.58\n",
      "[502 | 1643.17] loss=2.24 avg=2.58\n",
      "[503 | 1645.91] loss=2.52 avg=2.58\n",
      "[504 | 1648.67] loss=2.12 avg=2.58\n",
      "[505 | 1651.42] loss=1.90 avg=2.57\n",
      "[506 | 1654.15] loss=2.58 avg=2.57\n",
      "[507 | 1657.10] loss=2.41 avg=2.57\n",
      "[508 | 1660.04] loss=2.31 avg=2.56\n",
      "[509 | 1662.80] loss=2.42 avg=2.56\n",
      "[510 | 1665.54] loss=2.50 avg=2.56\n",
      "[511 | 1668.27] loss=2.30 avg=2.56\n",
      "[512 | 1671.02] loss=2.30 avg=2.56\n",
      "[513 | 1673.76] loss=2.61 avg=2.56\n",
      "[514 | 1676.51] loss=2.91 avg=2.56\n",
      "[515 | 1679.27] loss=2.39 avg=2.56\n",
      "[516 | 1682.01] loss=2.35 avg=2.56\n",
      "[517 | 1684.78] loss=2.72 avg=2.56\n",
      "[518 | 1687.54] loss=2.35 avg=2.56\n",
      "[519 | 1690.30] loss=2.26 avg=2.55\n",
      "[520 | 1693.04] loss=2.35 avg=2.55\n",
      "[521 | 1695.80] loss=2.15 avg=2.55\n",
      "[522 | 1698.56] loss=2.44 avg=2.55\n",
      "[523 | 1701.30] loss=2.91 avg=2.55\n",
      "[524 | 1704.04] loss=2.53 avg=2.55\n",
      "[525 | 1706.79] loss=2.76 avg=2.55\n",
      "[526 | 1709.54] loss=2.66 avg=2.55\n",
      "[527 | 1712.28] loss=2.44 avg=2.55\n",
      "[528 | 1715.03] loss=2.83 avg=2.56\n",
      "[529 | 1717.77] loss=2.94 avg=2.56\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[530 | 1720.51] loss=2.23 avg=2.56\n",
      "[531 | 1723.24] loss=2.34 avg=2.55\n",
      "[532 | 1725.98] loss=2.55 avg=2.55\n",
      "[533 | 1728.74] loss=2.28 avg=2.55\n",
      "[534 | 1731.49] loss=2.14 avg=2.55\n",
      "[535 | 1734.25] loss=2.60 avg=2.55\n",
      "[536 | 1736.98] loss=2.27 avg=2.54\n",
      "[537 | 1739.74] loss=2.31 avg=2.54\n",
      "[538 | 1742.50] loss=2.25 avg=2.54\n",
      "[539 | 1745.25] loss=2.89 avg=2.54\n",
      "[540 | 1748.00] loss=2.87 avg=2.55\n",
      "[541 | 1750.74] loss=2.42 avg=2.54\n",
      "[542 | 1753.48] loss=2.52 avg=2.54\n",
      "[543 | 1756.24] loss=2.56 avg=2.54\n",
      "[544 | 1758.99] loss=2.09 avg=2.54\n",
      "[545 | 1761.73] loss=2.14 avg=2.54\n",
      "[546 | 1764.46] loss=2.62 avg=2.54\n",
      "[547 | 1767.20] loss=2.74 avg=2.54\n",
      "[548 | 1769.95] loss=2.21 avg=2.54\n",
      "[549 | 1772.70] loss=2.18 avg=2.53\n",
      "[550 | 1775.44] loss=2.31 avg=2.53\n",
      "[551 | 1778.20] loss=2.21 avg=2.53\n",
      "[552 | 1780.95] loss=1.83 avg=2.52\n",
      "[553 | 1783.70] loss=2.45 avg=2.52\n",
      "[554 | 1786.45] loss=2.31 avg=2.52\n",
      "[555 | 1789.18] loss=2.79 avg=2.52\n",
      "[556 | 1791.93] loss=2.03 avg=2.51\n",
      "[557 | 1794.68] loss=2.26 avg=2.51\n",
      "[558 | 1797.42] loss=2.52 avg=2.51\n",
      "[559 | 1800.17] loss=2.07 avg=2.51\n",
      "[560 | 1802.91] loss=2.33 avg=2.51\n",
      "[561 | 1805.66] loss=2.45 avg=2.51\n",
      "[562 | 1808.41] loss=2.09 avg=2.50\n",
      "[563 | 1811.16] loss=2.25 avg=2.50\n",
      "[564 | 1813.92] loss=2.42 avg=2.50\n",
      "[565 | 1816.65] loss=2.69 avg=2.50\n",
      "[566 | 1819.39] loss=2.20 avg=2.50\n",
      "[567 | 1822.13] loss=2.45 avg=2.50\n",
      "[568 | 1824.85] loss=2.42 avg=2.50\n",
      "[569 | 1827.62] loss=2.56 avg=2.50\n",
      "[570 | 1830.36] loss=2.08 avg=2.49\n",
      "[571 | 1833.10] loss=2.32 avg=2.49\n",
      "[572 | 1835.89] loss=2.21 avg=2.49\n",
      "[573 | 1838.87] loss=2.07 avg=2.48\n",
      "[574 | 1841.76] loss=2.37 avg=2.48\n",
      "[575 | 1844.49] loss=2.46 avg=2.48\n",
      "[576 | 1847.24] loss=2.50 avg=2.48\n",
      "[577 | 1850.01] loss=2.18 avg=2.48\n",
      "[578 | 1852.75] loss=2.26 avg=2.48\n",
      "[579 | 1855.53] loss=2.39 avg=2.48\n",
      "[580 | 1858.30] loss=2.49 avg=2.48\n",
      "[581 | 1861.04] loss=2.16 avg=2.47\n",
      "[582 | 1863.77] loss=1.87 avg=2.47\n",
      "[583 | 1866.53] loss=2.13 avg=2.46\n",
      "[584 | 1869.27] loss=2.70 avg=2.47\n",
      "[585 | 1872.03] loss=1.60 avg=2.46\n",
      "[586 | 1874.76] loss=2.50 avg=2.46\n",
      "[587 | 1877.52] loss=2.16 avg=2.45\n",
      "[588 | 1880.27] loss=2.48 avg=2.46\n",
      "[589 | 1883.02] loss=2.16 avg=2.45\n",
      "[590 | 1885.78] loss=2.55 avg=2.45\n",
      "[591 | 1888.54] loss=1.81 avg=2.45\n",
      "[592 | 1891.30] loss=2.31 avg=2.45\n",
      "[593 | 1894.06] loss=2.19 avg=2.44\n",
      "[594 | 1896.81] loss=2.29 avg=2.44\n",
      "[595 | 1899.55] loss=2.29 avg=2.44\n",
      "[596 | 1902.31] loss=2.63 avg=2.44\n",
      "[597 | 1905.07] loss=2.14 avg=2.44\n",
      "[598 | 1907.82] loss=2.27 avg=2.44\n",
      "[599 | 1910.55] loss=2.25 avg=2.43\n",
      "Generating samples...\n",
      "======== SAMPLE 1 ========\n",
      " daughter of a person�️ \n",
      "\n",
      "Your beautiful! \n",
      "&amp;#x200B;\n",
      "\n",
      "This is a new chapter that I’ve ever been in my life, and even though it has a few days later and that’s usually a very good feeling, I’m so close to it now. I’ve been there. \n",
      "\n",
      "I truly believe that the rest of this life will be a more good and happier future for you and I’s still just as much confident you can be. I just want you to know that I care very much about you :) I truly do. \n",
      "\n",
      "&amp;#x200B;\n",
      "\n",
      "No, that’s just the thing. \n",
      "\n",
      "&amp;#x200B;\n",
      "\n",
      "You look like you’re absolutely gorgeous. And I’m proud of you. \n",
      "\n",
      "Don’t give up on yourself, and remember you’re worth it. Just keep pushing on, that’s something I’m always striving to do. \n",
      "\n",
      "I wish you nothing but the best for the upcoming year of your life.\"\n",
      "\"Just got roasted in the process of processing so I guess I’ve been off to work from the dead for 2nd time and it’s time to get back to the roast. Toast me?\" \"I'm very sorry for the horrible things you're going through! I'm happy you are doing your best, and I am so sorry how it feels!\"\n",
      "\"I’m a teen feeling insecure and lonely. Ive been self harming to ease the emotional pain im feeling.\" \"You look kind and sweet! You look like a very approachable person who is looking for some positivity to help me feel a bit better about my looks. \n",
      "\n",
      "Just read your profile on the page, then you'll have to wait up time until things get better because I'm worried you'll do well in your exams and your life will eventually get better :)\n",
      "\n",
      "Ps. You're young, I'm a musician. Maybe you're gonna be able to get into an orchestra... I'm not that far right now. (You're gonna find an orchestra soon, which I'm currently playing) and I'm certain that you would definitely be amazing. Keep your head up :)\"\n",
      "\"Going back to school Monday after brief 4 year intermission. I haven’t smoked a cigarette in 52.5 days!! And I deleted all social media 2 months ago. I'm 22(tm). I already have more social anxiety than most people do!! Anyone have some advice?\" \"My dad fought my dad back with a martial arts coach, the same people who beat him 3 times in 11 years.\n",
      "\n",
      "\n",
      "At the same time, my dad was diagnosed with epilepsy and I realized that I wasn't at all a normal person! So we were together and I realized that I had a few problems with my ability to love and understand how to deal with my illness. \n",
      "\n",
      "As a matter of fact, I did the same thing as many of my kids, but I realized that I too had just as many social issues as the medical profession, and that I didn’t make enough time to deal with what I was. I did however, it was very important to make some time for the medication. I was a good parent for some of my children, but my only relationship was an engagement, which meant all of my kids loved me and my grandfather. \n",
      "\n",
      "In the meantime, you should ask for a medication that will help you be confident with your anxiety and depression. I’m doing a lot of work with medication, especially at age 35(now 33). I’m fairly sure that will help me along along, but let me know if you need to talk. \n",
      "\n",
      "I’d be an immediate commenter on your profile if you need it. I’ve gotten a lot of help and many positive comments on my profile and social media. I feel it takes a lot of courage to vent, so I just had nothing to worry about. I feel very confident that you can find the strength to pull through this. (I’m sure you look like a great person.) and if anyone really cares about you, I can’t think of the good time in your life, my inbox is always open and ready to be where you are today.\"\n",
      "\"After 3 weeks of being diagnosed with a very life-altering illness and very low self esteem, I’m back single, with 3 kids, trying to find ways to make a life for me. I feel pretty anxious and depressed, could use some positivity\" \"You look like a man! A man! Don’t listen to your brain telling you are one to be depressed, it’s not a lie. I’m sure you deserve it and you deserve it right now. Keep pushing\n",
      "\n",
      "[600 | 1960.29] loss=2.11 avg=2.43\n",
      "[601 | 1963.05] loss=2.40 avg=2.43\n",
      "[602 | 1965.79] loss=2.69 avg=2.43\n",
      "[603 | 1968.55] loss=2.46 avg=2.43\n",
      "[604 | 1971.30] loss=1.99 avg=2.43\n",
      "[605 | 1974.07] loss=2.24 avg=2.43\n",
      "[606 | 1976.81] loss=2.49 avg=2.43\n",
      "[607 | 1979.56] loss=2.94 avg=2.43\n",
      "[608 | 1982.31] loss=2.26 avg=2.43\n",
      "[609 | 1985.06] loss=2.51 avg=2.43\n",
      "[610 | 1987.81] loss=2.18 avg=2.43\n",
      "[611 | 1990.54] loss=2.23 avg=2.43\n",
      "[612 | 1993.29] loss=2.39 avg=2.43\n",
      "[613 | 1996.05] loss=2.49 avg=2.43\n",
      "[614 | 1998.77] loss=2.46 avg=2.43\n",
      "[615 | 2001.53] loss=1.93 avg=2.42\n",
      "[616 | 2004.28] loss=2.18 avg=2.42\n",
      "[617 | 2007.02] loss=2.38 avg=2.42\n",
      "[618 | 2009.76] loss=2.27 avg=2.42\n",
      "[619 | 2012.50] loss=2.24 avg=2.42\n",
      "[620 | 2015.25] loss=2.46 avg=2.42\n",
      "[621 | 2018.25] loss=2.44 avg=2.42\n",
      "[622 | 2021.14] loss=1.79 avg=2.41\n",
      "[623 | 2023.88] loss=2.32 avg=2.41\n",
      "[624 | 2026.63] loss=1.77 avg=2.40\n",
      "[625 | 2029.38] loss=2.25 avg=2.40\n",
      "[626 | 2032.14] loss=2.07 avg=2.40\n",
      "[627 | 2034.88] loss=2.60 avg=2.40\n",
      "[628 | 2037.63] loss=2.38 avg=2.40\n",
      "[629 | 2040.38] loss=2.37 avg=2.40\n",
      "[630 | 2043.12] loss=2.57 avg=2.40\n",
      "[631 | 2045.86] loss=1.96 avg=2.40\n",
      "[632 | 2048.63] loss=2.31 avg=2.40\n",
      "[633 | 2051.38] loss=2.25 avg=2.40\n",
      "[634 | 2054.12] loss=2.25 avg=2.39\n",
      "[635 | 2056.86] loss=2.34 avg=2.39\n",
      "[636 | 2059.60] loss=2.09 avg=2.39\n",
      "[637 | 2062.32] loss=2.49 avg=2.39\n",
      "[638 | 2065.07] loss=2.18 avg=2.39\n",
      "[639 | 2067.82] loss=2.25 avg=2.39\n",
      "[640 | 2070.55] loss=2.14 avg=2.39\n",
      "[641 | 2073.31] loss=1.98 avg=2.38\n",
      "[642 | 2076.05] loss=2.57 avg=2.38\n",
      "[643 | 2078.81] loss=2.06 avg=2.38\n",
      "[644 | 2081.55] loss=2.43 avg=2.38\n",
      "[645 | 2084.29] loss=2.39 avg=2.38\n",
      "[646 | 2087.05] loss=2.21 avg=2.38\n",
      "[647 | 2089.80] loss=2.05 avg=2.38\n",
      "[648 | 2092.55] loss=1.59 avg=2.37\n",
      "[649 | 2095.29] loss=1.91 avg=2.36\n",
      "[650 | 2098.04] loss=2.88 avg=2.37\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[651 | 2100.78] loss=2.23 avg=2.37\n",
      "[652 | 2103.54] loss=1.54 avg=2.36\n",
      "[653 | 2106.29] loss=2.44 avg=2.36\n",
      "[654 | 2109.05] loss=1.85 avg=2.35\n",
      "[655 | 2111.81] loss=2.34 avg=2.35\n",
      "[656 | 2114.56] loss=2.03 avg=2.35\n",
      "[657 | 2117.30] loss=1.88 avg=2.35\n",
      "[658 | 2120.06] loss=2.02 avg=2.34\n",
      "[659 | 2122.81] loss=2.14 avg=2.34\n",
      "[660 | 2125.56] loss=2.19 avg=2.34\n",
      "[661 | 2128.33] loss=2.31 avg=2.34\n",
      "[662 | 2131.10] loss=2.36 avg=2.34\n",
      "[663 | 2133.85] loss=2.29 avg=2.34\n",
      "[664 | 2136.59] loss=1.87 avg=2.33\n",
      "[665 | 2139.33] loss=2.40 avg=2.34\n",
      "[666 | 2142.06] loss=2.53 avg=2.34\n",
      "[667 | 2144.83] loss=2.13 avg=2.34\n",
      "[668 | 2147.57] loss=2.25 avg=2.33\n",
      "[669 | 2150.32] loss=2.46 avg=2.34\n",
      "[670 | 2153.05] loss=4.24 avg=2.35\n",
      "[671 | 2155.82] loss=2.46 avg=2.36\n",
      "[672 | 2158.56] loss=2.42 avg=2.36\n",
      "[673 | 2161.31] loss=2.12 avg=2.35\n",
      "[674 | 2164.05] loss=2.81 avg=2.36\n",
      "[675 | 2166.78] loss=2.43 avg=2.36\n",
      "[676 | 2169.53] loss=2.25 avg=2.36\n",
      "[677 | 2172.28] loss=2.24 avg=2.36\n",
      "[678 | 2175.03] loss=2.45 avg=2.36\n",
      "[679 | 2177.78] loss=2.68 avg=2.36\n",
      "[680 | 2180.52] loss=2.46 avg=2.36\n",
      "[681 | 2183.26] loss=2.40 avg=2.36\n",
      "[682 | 2186.03] loss=2.48 avg=2.36\n",
      "[683 | 2188.80] loss=2.19 avg=2.36\n",
      "[684 | 2191.56] loss=2.56 avg=2.36\n",
      "[685 | 2194.30] loss=2.43 avg=2.36\n",
      "[686 | 2197.24] loss=1.90 avg=2.36\n",
      "[687 | 2200.19] loss=2.58 avg=2.36\n",
      "[688 | 2202.94] loss=2.83 avg=2.37\n",
      "[689 | 2205.69] loss=2.24 avg=2.37\n",
      "[690 | 2208.44] loss=2.65 avg=2.37\n",
      "[691 | 2211.21] loss=2.16 avg=2.37\n",
      "[692 | 2213.97] loss=2.86 avg=2.37\n",
      "[693 | 2216.74] loss=2.06 avg=2.37\n",
      "[694 | 2219.50] loss=2.71 avg=2.37\n",
      "[695 | 2222.26] loss=2.27 avg=2.37\n",
      "[696 | 2225.01] loss=1.98 avg=2.37\n",
      "[697 | 2227.75] loss=2.05 avg=2.36\n",
      "[698 | 2230.51] loss=2.37 avg=2.36\n",
      "[699 | 2233.26] loss=2.11 avg=2.36\n",
      "Generating samples...\n",
      "======== SAMPLE 1 ========\n",
      " the a guy who deserves love and affection even if he doesn't know it yet\"\n",
      "\"I had a pretty bad surgery a couple of days ago. It did get better, but I was left without my favourite type of glasses on. Could use a good toast!\" \"It is ok and your glasses are perfect as well.\"\n",
      "\"24M - Been feeling down lately. Could use a good toast\" \"you are a very nice-looking guy, and i’m not going to lie.\n",
      "\n",
      "your eyes are gorgeous, and your hair is so cute. i think you have some wonderful, unusual, unique talents. you have the whole universe at your fingertips, with people making your life a wonderful place on the internet, and being able to listen to whatever you listen for!\"\n",
      "\"My girlfriend has been battling breast cancer for nearly 10 years, and I’m just having an off day. I’d appreciate any kind words from people with the same weight, I get lonely every now and then and I miss my job (never get tired and not happy) the majority of the time\" \"You are a light, very, very young woman.\"\n",
      "\"I had a miscarriage and was pregnant for it. Life’s been getting bleh lately. I’m struggling with my self image. Could use a toast to boost my confidence a little bit.\" \"I'm so proud of you in your effort to carry out the best wishes. I am so proud of you that you are doing well, and I hope you have an amazing life ahead of you.\n",
      "\n",
      "Pregnancy sucks, but it's all worth while. Don't let anyone get you down sometimes, and don't let anyone treat you poorly. Try to be positive, and give yourself an energy and support your way to achieving your goal.\n",
      "\n",
      "And lastly, remember that you can continue doing these things, because you're doing so great at them. Also, all you really need to do is go outside of this too-complicated.\n",
      "\n",
      "I think having the balls to conceive is good and good and the best thing.\"\n",
      "\"22, very single, and almost single for the fourth time. Toast me please.\" \"I'm so sorry you're going through it, but the loneliness is coming. Try to remember that when you try to make a small move, things get bigger, and you realise that you're going to miss your opportunity and be a better person.\n",
      "\n",
      "Please know that when life's at it, it's always at your fingertips and you can see your future and see it in them, like sunshine in their green eyes.\n",
      "\n",
      "You deserve the best of the future. We all care for you. If you are single, give yourself time, and do the things you want for your own well and well. When a person's in the end, they know you’re in control of their own happiness and fulfillment. You deserve to be happy.\n",
      "\n",
      "Don’t ever forget to smile. You only get as far as you want from failure. You only get as far to a happier ending and as far as you can go to achieve the success.\n",
      "\n",
      "You can make it through this hard journey and achieve nothing at all, you've found the means, people to take time for you are probably better with your health and life and they've found all the motivation to keep going. You just have to find people and start going for it to the heart.\n",
      "\n",
      "You are more than beautiful and your smile will bloom with time. You are in a position where you are ready for a whole new life and it will just get better until you feel the strength and courage you can.\n",
      "\n",
      "Keep your smile. It's your life. We'll be there for you.\"\n",
      "\"Been feeling down. Depressive. Letting my depression go. Been single for like 6 years, self esteem is 0. Help me feel better at the beach with a nice hot day and a nice conversation? Thanks.\" \"I'm sorry you're feeling down, you're not lonely at all, you're just looking for fun. I know I have your feelings, it's ok to take a moment to tell you're not alone. You have to accept who you are and feel the best.\n",
      "\n",
      "Edit: added my thoughts on your first post.\"\n",
      "\"I feel like I can’t get a picture right and basically anything that could make me feel bad\" \"Your face is so warm and pretty! You are gorgeous and I'd love to date you.\"\n",
      "\"I have a horrible childhood and I’m still a virgin. I feel so unattractive and unworthy of anyone. I feel like I’m not worthy of love. I feel like I’m not worth my time. I don’t understand why people choose to date me when I don’t even want to. I don’t know what they are going through, but I remember you asking for\n",
      "\n",
      "[700 | 2283.52] loss=2.51 avg=2.36\n",
      "[701 | 2286.28] loss=2.73 avg=2.37\n",
      "[702 | 2289.02] loss=2.33 avg=2.37\n",
      "[703 | 2291.76] loss=2.41 avg=2.37\n",
      "[704 | 2294.50] loss=2.11 avg=2.36\n",
      "[705 | 2297.25] loss=2.28 avg=2.36\n",
      "[706 | 2300.00] loss=2.39 avg=2.36\n",
      "[707 | 2302.77] loss=2.46 avg=2.36\n",
      "[708 | 2305.50] loss=2.44 avg=2.36\n",
      "[709 | 2308.27] loss=2.29 avg=2.36\n",
      "[710 | 2311.02] loss=1.81 avg=2.36\n",
      "[711 | 2313.76] loss=2.28 avg=2.36\n",
      "[712 | 2316.50] loss=2.33 avg=2.36\n",
      "[713 | 2319.26] loss=2.38 avg=2.36\n",
      "[714 | 2322.00] loss=2.12 avg=2.36\n",
      "[715 | 2324.74] loss=2.17 avg=2.35\n",
      "[716 | 2327.49] loss=2.38 avg=2.35\n",
      "[717 | 2330.26] loss=2.20 avg=2.35\n",
      "[718 | 2333.02] loss=2.24 avg=2.35\n",
      "[719 | 2335.77] loss=2.45 avg=2.35\n",
      "[720 | 2338.51] loss=2.05 avg=2.35\n",
      "[721 | 2341.26] loss=2.29 avg=2.35\n",
      "[722 | 2344.01] loss=2.23 avg=2.35\n",
      "[723 | 2346.76] loss=2.56 avg=2.35\n",
      "[724 | 2349.50] loss=2.19 avg=2.35\n",
      "[725 | 2352.25] loss=2.57 avg=2.35\n",
      "[726 | 2355.02] loss=2.58 avg=2.35\n",
      "[727 | 2357.77] loss=1.82 avg=2.35\n",
      "[728 | 2360.52] loss=2.21 avg=2.35\n",
      "[729 | 2363.27] loss=2.41 avg=2.35\n",
      "[730 | 2366.02] loss=2.52 avg=2.35\n",
      "[731 | 2368.79] loss=2.00 avg=2.34\n",
      "[732 | 2371.54] loss=1.96 avg=2.34\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "!python train.py --dataset test2.npz --learning_rate 0.0005 --batch_size 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This code comes from the generate interactive conditional file \n",
    "class GPT2:\n",
    "\n",
    "  \n",
    "  # extracted from the source code to generate some text based on a prior\n",
    "  def __init__(\n",
    "      self,\n",
    "      model_name='117M',\n",
    "      seed=None,\n",
    "      nsamples=1,\n",
    "      batch_size=1,\n",
    "      length=None,\n",
    "      temperature=.95,\n",
    "      top_k=0,\n",
    "      raw_text=\"\",\n",
    "  ):\n",
    "      \"\"\"\n",
    "      Interactively run the model\n",
    "      :model_name=117M : String, which model to use\n",
    "      :seed=None : Integer seed for random number generators, fix seed to reproduce\n",
    "       results\n",
    "      :nsamples=1 : Number of samples to return total\n",
    "      :batch_size=1 : Number of batches (only affects speed/memory).  Must divide nsamples.\n",
    "      :length=None : Number of tokens in generated text, if None (default), is\n",
    "       determined by model hyperparameters\n",
    "      :temperature=1 : Float value controlling randomness in boltzmann\n",
    "       distribution. Lower temperature results in less random completions. As the\n",
    "       temperature approaches zero, the model will become deterministic and\n",
    "       repetitive. Higher temperature results in more random completions.\n",
    "      :top_k=0 : Integer value controlling diversity. 1 means only 1 word is\n",
    "       considered for each step (token), resulting in deterministic completions,\n",
    "       while 40 means 40 words are considered at each step. 0 (default) is a\n",
    "       special setting meaning no restrictions. 40 generally is a good value.\n",
    "      \"\"\"\n",
    "      if batch_size is None:\n",
    "          batch_size = 1\n",
    "      assert nsamples % batch_size == 0\n",
    "\n",
    "      self.nsamples = nsamples\n",
    "      self.batch_size = batch_size\n",
    "      \n",
    "      self.enc = encoder.get_encoder(model_name)\n",
    "      hparams = model.default_hparams()\n",
    "      with open(os.path.join('models', model_name, 'hparams.json')) as f:\n",
    "          hparams.override_from_dict(json.load(f))\n",
    "\n",
    "      if length is None:\n",
    "          length = hparams.n_ctx // 2\n",
    "      elif length > hparams.n_ctx:\n",
    "          raise ValueError(\"Can't get samples longer than window size: %s\" % hparams.n_ctx)\n",
    "\n",
    "      self.sess = tf.Session(graph=tf.Graph())\n",
    "      self.sess.__enter__()\n",
    "      \n",
    "      self.context = tf.placeholder(tf.int32, [batch_size, None])\n",
    "      np.random.seed(seed)\n",
    "      tf.set_random_seed(seed)\n",
    "      self.output = sample.sample_sequence(\n",
    "          hparams=hparams, length=length,\n",
    "          context=self.context,\n",
    "          batch_size=batch_size,\n",
    "          temperature=temperature, top_k=top_k\n",
    "      )\n",
    "\n",
    "      saver = tf.train.Saver()\n",
    "      self.ckpt = tf.train.latest_checkpoint(os.path.join('models', model_name))\n",
    "      saver.restore(self.sess, self.ckpt)\n",
    "\n",
    "  def close(self):\n",
    "    self.sess.close()\n",
    "  \n",
    "  def generate_conditional(self,raw_text):\n",
    "      context_tokens = self.enc.encode(raw_text)\n",
    "      generated = 0\n",
    "      for _ in range(self.nsamples // self.batch_size):\n",
    "          out = self.sess.run(self.output, feed_dict={\n",
    "              self.context: [context_tokens for _ in range(self.batch_size)]\n",
    "          })[:, len(context_tokens):]\n",
    "          for i in range(self.batch_size):\n",
    "              generated += 1\n",
    "              text = self.enc.decode(out[i])\n",
    "              return text\n",
    "              #print(\"=\" * 40 + \" SAMPLE \" + str(generated) + \" \" + \"=\" * 40)\n",
    "              #print(text)\n",
    "      #print(\"=\" * 80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ec2-user/SageMaker/gpt-2/model.py:148: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ec2-user/SageMaker/gpt-2/model.py:152: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ec2-user/SageMaker/gpt-2/model.py:36: The name tf.rsqrt is deprecated. Please use tf.math.rsqrt instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ec2-user/SageMaker/gpt-2/model.py:166: The name tf.add_to_collection is deprecated. Please use tf.compat.v1.add_to_collection instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ec2-user/SageMaker/gpt-2/sample.py:65: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.cast` instead.\n",
      "WARNING:tensorflow:From /home/ec2-user/SageMaker/gpt-2/sample.py:70: multinomial (from tensorflow.python.ops.random_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.random.categorical` instead.\n",
      "INFO:tensorflow:Restoring parameters from models/117M/model.ckpt\n"
     ]
    }
   ],
   "source": [
    "gpt2 = GPT2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gpt2.close() #remember to close session after\n",
    "\n",
    "#try updating deprecated tensorflow functions in model.py if necessary (or use Docker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\"\n",
      "\n",
      "You ask if it is safe to question the codeword cherishing inside him.\n",
      "\n",
      "\"What gives girl you're here? Meet my old friend Krumkov? Someone from your past? Cause he screws the wrong people. Tell him, he should reach the Hotel talli Ive no doubt about it, it's nothing to kill for.\"\n",
      "\n",
      "Penitent looks up at you, wide-eyed. \"Galp...\"\n",
      "\n",
      "Search Warrant Pay\n",
      "\n",
      "Darius Kurmov:\n",
      "\n",
      "An Archaeologist and Sydney Esperang\n",
      "\n",
      "There is major public activity on Piltover 27 in the form of an intense wave of actions. The simultaneous hijacking of all cargo ships, smuggling and reconnaissance operations linking Piltover with Germany and Tehran, is temporary and ineffectual. Fighters from Kazortas each declared for surgical mutiny, while engineers charged with bringing care into operation appear as helpless victims of pireface without fingers and who roam through vivid oxymorons left out of dinner talk. The City's AI is activated, except by Crossing Over. Inquirer Nolf refuses to utter the words of unimaginable grief to users, now gathered in their tiny spaces, in deserted pubs, parks and malls, Rettens and shopping centres - and is convinced the (very public) kidnap of the Athena overturned the timestream: hostilities have too savage a tone to hold the last two for more than partly emotional episode padding. Complains Lycan Thontrox 25, Ekkerothious informs visitor Ron Patvan - a veteran idealist agreed to return to a blood feud due to intractable debt - that they need urgently to update their infographics. This content is running.\n",
      "\n",
      "Ecology Occult\n",
      "\n",
      "What Can We Tell About Gods From The New Hideous Origins?\n",
      "\n",
      "Ingrin Skarkov:<|endoftext|>Daylight Savings Time is a new time; for the revived Bitcoin community, perhaps the get-robot is finally emerging as a real thing that can play out between one service provider or another.\n",
      "\n",
      "A coworker of mine recently sent me a small list of three the devilish capabilities of this solar phone installation, ignoring the fact that there are many devices out there improving on fixtures like these that, due to the insufficient surface area, are difficult to analyze.\n",
      "\n",
      "Landline Cables\n",
      "\n",
      "1. Outgoing Peer Time\n",
      "\n",
      "Obviously, we are not having a heated argument over this. In fact, it's pretty much its state. People while entitled\n"
     ]
    }
   ],
   "source": [
    "# result = gpt2.generate_conditional(raw_text=\"Hello bot, tell me about yourself\") #chatbot logic should work if i put this inside class\n",
    "\n",
    "# print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Who:\n",
    "    \"\"\"A class defining the conversation parties: me, friend\"\"\"\n",
    "    def __init__(self):\n",
    "        self.prefixes = []\n",
    "\n",
    "    def matches(self,phrase):\n",
    "        for prefix in self.prefixes:\n",
    "            if phrase.startswith(prefix):\n",
    "            #print(f\"{phrase} starts with {prefix}\")\n",
    "                return True\n",
    "\n",
    "    #print(f\"{phrase} does not start with {self.prefixes}\")\n",
    "        return False\n",
    "\n",
    "    def get_random_prefix(self):\n",
    "        return self.prefixes[0] #just use first one\n",
    "\n",
    "class Friend(Who):\n",
    "    def __init__(self):\n",
    "        super().__init__()  #initializes prefixes from who\n",
    "        self.prefixes = [\"Friend said: \\\"\"]\n",
    "   \n",
    "  \n",
    "class You(Who):\n",
    "    def __init__(self):\n",
    "        super().__init__() \n",
    "        self.prefixes = [\"You said: \\\"\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Chatter:\n",
    "  \n",
    "    def __init__(self, prior = None):\n",
    "        if prior is None:\n",
    "            prior=\"\"\"\n",
    "              You: \"Hello, are you my friend?\"\n",
    "              Friend: \"Of course!\"\n",
    "              \"\"\"\n",
    "        self.suggestion = None\n",
    "\n",
    "        self.friend = Friend()\n",
    "        self.you = You()\n",
    "        self.parties  = [ self.friend, self.you ]\n",
    "\n",
    "        self.conversation = []\n",
    "\n",
    "        lines = prior.split(\"\\n\")\n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            if len(line)!=0:\n",
    "                party = None\n",
    "                for party in self.parties:\n",
    "                    if party.matches(line):\n",
    "                        break\n",
    "                if party is None:\n",
    "                    raise Exception(f\"Unknown party: {line}\")\n",
    "                self.conversation.append((party,line))\n",
    "        self.get_suggestion()\n",
    "\n",
    "\n",
    "    def get_prior(self):\n",
    "        conv = \"\"\n",
    "        for (party, line) in self.conversation:\n",
    "            conv+=line+\"\\n\"\n",
    "        return conv\n",
    "\n",
    "    def get_suggestion(self):\n",
    "        who, last_line = self.conversation[-1]\n",
    "\n",
    "        party_index = self.parties.index(who)\n",
    "        next_party = self.parties[(party_index+1) % len(self.parties)]\n",
    "\n",
    "        conv = self.get_prior()\n",
    "        conv += next_party.get_random_prefix()\n",
    "        answer = self.get_answer(next_party, conv)\n",
    "\n",
    "        if not next_party.matches(answer):\n",
    "            prefix = next_party.get_random_prefix()\n",
    "            answer = prefix + answer\n",
    "\n",
    "        self.suggestion = (next_party, answer)\n",
    "    def show(self):\n",
    "        conv = \"\"\n",
    "        for (party, line) in self.conversation:\n",
    "            conv+=line+\"\\n\"\n",
    "        print(conv)\n",
    "        if self.suggestion is not None:\n",
    "            party, answer  = self.suggestion\n",
    "            print(answer)\n",
    "\n",
    "\n",
    "    def next(self, party = None, answer = \"\"):\n",
    "        \"\"\"Continue the conversation\n",
    "        param party: None -> use the current party which is currently in turn\n",
    "        param answer: override the suggestion\n",
    "\n",
    "        \"\"\"\n",
    "        suggested_party, suggested_answer = self.suggestion\n",
    "        if party is None:\n",
    "            party = suggested_party\n",
    "\n",
    "        if answer == \"\":\n",
    "            answer = suggested_answer\n",
    "\n",
    "        if not party.matches(answer):\n",
    "            prefix = party.get_random_prefix()\n",
    "            answer = prefix + answer\n",
    "\n",
    "        answer = answer.strip()\n",
    "        \n",
    "       \n",
    "        if answer[-1] != \"\\\"\":\n",
    "            answer += \"\\\"\"\n",
    "         # add the closing \"\n",
    "\n",
    "        self.conversation.append((party, answer))    \n",
    "        self.get_suggestion()\n",
    "        self.show()\n",
    "\n",
    "    def retry(self):\n",
    "        self.get_suggestion()\n",
    "\n",
    "    def get_answer(self, party, conv):\n",
    "        answer = gpt2.generate_conditional(raw_text=conv)\n",
    "        lines = answer.split(\"\\n\")\n",
    "        line = \"\"\n",
    "        for line in lines:\n",
    "            if line !=\"\":\n",
    "                break\n",
    "\n",
    "\n",
    "        if line!=\"\":\n",
    "            return line\n",
    "\n",
    "        return \"\"\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = Chatter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You: \"Hello, are you my friend?\"\n",
      "Friend: \"Of course!\"\n",
      "\n",
      "--> Friend said: \"Please trust me.\"\n"
     ]
    }
   ],
   "source": [
    "c.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "c.retry()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "c.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "c.next(c.you, \"\\\"13m I feel sad I am ugly\\\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You: \"Hello, are you my friend?\"\n",
      "Friend: \"Of course!\"\n",
      "You said: \"\"13m I feel sad I am ugly\"\n",
      "\n",
      "Friend said: \"You are hardworking, Chris...\"\n"
     ]
    }
   ],
   "source": [
    "c.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#c.next(c.you, \"\\\"My girlfriend left me and my mom was abusive toast me\\\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt2.close() #remember to close session after"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_p36",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
