{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONTRIBUTORS.md  download_model.py  requirements.txt  Untitled.ipynb\r\n",
      "DEVELOPERS.md\t encode.py\t    src\r\n",
      "Dockerfile.cpu\t LICENSE\t    train-horovod.py\r\n",
      "Dockerfile.gpu\t README.md\t    train.py\r\n"
     ]
    }
   ],
   "source": [
    "!ls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting fire>=0.1.3\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d9/69/faeaae8687f4de0f5973694d02e9d6c3eb827636a009157352d98de1129e/fire-0.2.1.tar.gz (76kB)\n",
      "\u001b[K     |████████████████████████████████| 81kB 11.7MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting regex==2017.4.5\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/36/62/c0c0d762ffd4ffaf39f372eb8561b8d491a11ace5a7884610424a8b40f95/regex-2017.04.05.tar.gz (601kB)\n",
      "\u001b[K     |████████████████████████████████| 604kB 27.0MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting requests==2.21.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/e3/20f3d364d6c8e5d2353c72a67778eb189176f08e873c9900e10c0287b84b/requests-2.21.0-py2.py3-none-any.whl (57kB)\n",
      "\u001b[K     |████████████████████████████████| 61kB 17.1MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tqdm==4.31.1\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6c/4b/c38b5144cf167c4f52288517436ccafefe9dc01b8d1c190e18a6b154cd4a/tqdm-4.31.1-py2.py3-none-any.whl (48kB)\n",
      "\u001b[K     |████████████████████████████████| 51kB 13.9MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting toposort==1.5\n",
      "  Downloading https://files.pythonhosted.org/packages/e9/8a/321cd8ea5f4a22a06e3ba30ef31ec33bea11a3443eeb1d89807640ee6ed4/toposort-1.5-py2.py3-none-any.whl\n",
      "Requirement already satisfied: six in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from fire>=0.1.3->-r requirements.txt (line 1)) (1.11.0)\n",
      "Requirement already satisfied: termcolor in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from fire>=0.1.3->-r requirements.txt (line 1)) (1.1.0)\n",
      "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from requests==2.21.0->-r requirements.txt (line 3)) (1.23)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from requests==2.21.0->-r requirements.txt (line 3)) (2019.9.11)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from requests==2.21.0->-r requirements.txt (line 3)) (2.6)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from requests==2.21.0->-r requirements.txt (line 3)) (3.0.4)\n",
      "Building wheels for collected packages: fire, regex\n",
      "  Building wheel for fire (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for fire: filename=fire-0.2.1-py2.py3-none-any.whl size=103543 sha256=38aafb71196bdc4923a7a62f1f448ba47f4bf3ada15d0658452aa7d306883f41\n",
      "  Stored in directory: /home/ec2-user/.cache/pip/wheels/31/9c/c0/07b6dc7faf1844bb4688f46b569efe6cafaa2179c95db821da\n",
      "  Building wheel for regex (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for regex: filename=regex-2017.4.5-cp36-cp36m-linux_x86_64.whl size=564360 sha256=a9b431b3b24c4ad99f0961522a26cb60d3a87cb154d6a6abd6d2efa27c367c30\n",
      "  Stored in directory: /home/ec2-user/.cache/pip/wheels/75/07/38/3c16b529d50cb4e0cd3dbc7b75cece8a09c132692c74450b01\n",
      "Successfully built fire regex\n",
      "Installing collected packages: fire, regex, requests, tqdm, toposort\n",
      "  Found existing installation: requests 2.20.0\n",
      "    Uninstalling requests-2.20.0:\n",
      "      Successfully uninstalled requests-2.20.0\n",
      "Successfully installed fire-0.2.1 regex-2017.4.5 requests-2.21.0 toposort-1.5 tqdm-4.31.1\n",
      "\u001b[33mWARNING: You are using pip version 19.3.1; however, version 20.0.2 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip3 install -r requirements.txt #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching checkpoint: 1.00kit [00:00, 1.10Mit/s]                                                     \n",
      "Fetching encoder.json: 1.04Mit [00:00, 44.5Mit/s]                                                   \n",
      "Fetching hparams.json: 1.00kit [00:00, 1.58Mit/s]                                                   \n",
      "Fetching model.ckpt.data-00000-of-00001: 1.42Git [00:18, 75.4Mit/s]                                 \n",
      "Fetching model.ckpt.index: 11.0kit [00:00, 12.8Mit/s]                                               \n",
      "Fetching model.ckpt.meta: 927kit [00:00, 42.8Mit/s]                                                 \n",
      "Fetching vocab.bpe: 457kit [00:00, 35.6Mit/s]                                                       \n"
     ]
    }
   ],
   "source": [
    "!python download_model.py 345M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "!export PYTHONIOENCODING=UTF-8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/home/ec2-user/SageMaker/gpt-2/src\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/SageMaker/gpt-2\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ec2-user/SageMaker/gpt-2/src/model.py:147: The name tf.AUTO_REUSE is deprecated. Please use tf.compat.v1.AUTO_REUSE instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import fire\n",
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import model, sample, encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching checkpoint: 1.00kit [00:00, 1.07Mit/s]                                                     \n",
      "Fetching encoder.json: 1.04Mit [00:00, 48.7Mit/s]                                                   \n",
      "Fetching hparams.json: 1.00kit [00:00, 1.33Mit/s]                                                   \n",
      "Fetching model.ckpt.data-00000-of-00001: 1.42Git [00:17, 79.7Mit/s]                                 \n",
      "Fetching model.ckpt.index: 11.0kit [00:00, 9.70Mit/s]                                               \n",
      "Fetching model.ckpt.meta: 927kit [00:00, 49.6Mit/s]                                                 \n",
      "Fetching vocab.bpe: 457kit [00:00, 37.3Mit/s]                                                       \n"
     ]
    }
   ],
   "source": [
    "import interactive_conditional_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#I copied the class out of src to here to modify it with visible ease, it can go back\n",
    "class GPT2:\n",
    "\n",
    "\n",
    "  # extracted from the source code to generate some text based on a prior\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name='345M',\n",
    "        seed=None,\n",
    "        nsamples=1,\n",
    "        batch_size=1,\n",
    "        length=None,\n",
    "        temperature=1,\n",
    "        top_k=0,\n",
    "        raw_text=\"\",\n",
    "  ):\n",
    "      \"\"\"\n",
    "      Interactively run the model\n",
    "      :model_name=117M : String, which model to use\n",
    "      :seed=None : Integer seed for random number generators, fix seed to reproduce\n",
    "       results\n",
    "      :nsamples=1 : Number of samples to return total\n",
    "      :batch_size=1 : Number of batches (only affects speed/memory).  Must divide nsamples.\n",
    "      :length=None : Number of tokens in generated text, if None (default), is\n",
    "       determined by model hyperparameters\n",
    "      :temperature=1 : Float value controlling randomness in boltzmann\n",
    "       distribution. Lower temperature results in less random completions. As the\n",
    "       temperature approaches zero, the model will become deterministic and\n",
    "       repetitive. Higher temperature results in more random completions.\n",
    "      :top_k=0 : Integer value controlling diversity. 1 means only 1 word is\n",
    "       considered for each step (token), resulting in deterministic completions,\n",
    "       while 40 means 40 words are considered at each step. 0 (default) is a\n",
    "       special setting meaning no restrictions. 40 generally is a good value.\n",
    "      \"\"\"\n",
    "        if batch_size is None:\n",
    "            batch_size = 1\n",
    "        assert nsamples % batch_size == 0\n",
    "\n",
    "        self.nsamples = nsamples\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.enc = encoder.get_encoder(model_name)\n",
    "        hparams = model.default_hparams()\n",
    "        with open(os.path.join('models', model_name, 'hparams.json')) as f:\n",
    "            hparams.override_from_dict(json.load(f))\n",
    "\n",
    "        if length is None:\n",
    "            length = hparams.n_ctx // 2\n",
    "        elif length > hparams.n_ctx:\n",
    "            raise ValueError(\"Can't get samples longer than window size: %s\" % hparams.n_ctx)\n",
    "\n",
    "        self.sess = tf.Session(graph=tf.Graph())\n",
    "        self.sess.__enter__()\n",
    "\n",
    "        self.context = tf.placeholder(tf.int32, [batch_size, None])\n",
    "        np.random.seed(seed)\n",
    "        tf.set_random_seed(seed)\n",
    "        self.output = sample.sample_sequence(\n",
    "          hparams=hparams, length=length,\n",
    "          context=self.context,\n",
    "          batch_size=batch_size,\n",
    "          temperature=temperature, top_k=top_k\n",
    "        )\n",
    "\n",
    "        saver = tf.train.Saver()\n",
    "        self.ckpt = tf.train.latest_checkpoint(os.path.join('models', model_name))\n",
    "        saver.restore(self.sess, self.ckpt)\n",
    "\n",
    "    def close(self):\n",
    "        self.sess.close()\n",
    "  \n",
    "    def generate_conditional(self,raw_text):\n",
    "        context_tokens = self.enc.encode(raw_text)\n",
    "        generated = 0\n",
    "        for _ in range(self.nsamples // self.batch_size):\n",
    "            out = self.sess.run(self.output, feed_dict={\n",
    "              self.context: [context_tokens for _ in range(self.batch_size)]\n",
    "            })[:, len(context_tokens):]\n",
    "            for i in range(self.batch_size):\n",
    "                generated += 1\n",
    "                text = self.enc.decode(out[i])\n",
    "                return text\n",
    "              #print(\"=\" * 40 + \" SAMPLE \" + str(generated) + \" \" + \"=\" * 40)\n",
    "              #print(text)\n",
    "        #print(\"=\" * 80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ec2-user/SageMaker/gpt-2/src/model.py:148: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ec2-user/SageMaker/gpt-2/src/model.py:152: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ec2-user/SageMaker/gpt-2/src/model.py:36: The name tf.rsqrt is deprecated. Please use tf.math.rsqrt instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ec2-user/SageMaker/gpt-2/src/model.py:166: The name tf.add_to_collection is deprecated. Please use tf.compat.v1.add_to_collection instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ec2-user/SageMaker/gpt-2/src/sample.py:65: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.cast` instead.\n",
      "WARNING:tensorflow:From /home/ec2-user/SageMaker/gpt-2/src/sample.py:70: multinomial (from tensorflow.python.ops.random_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.random.categorical` instead.\n",
      "INFO:tensorflow:Restoring parameters from models/345M/model.ckpt\n"
     ]
    }
   ],
   "source": [
    "gpt2 = GPT2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gpt2.close() #remember to close session after"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\"\n",
      "\n",
      "You ask if it is safe to question the codeword cherishing inside him.\n",
      "\n",
      "\"What gives girl you're here? Meet my old friend Krumkov? Someone from your past? Cause he screws the wrong people. Tell him, he should reach the Hotel talli Ive no doubt about it, it's nothing to kill for.\"\n",
      "\n",
      "Penitent looks up at you, wide-eyed. \"Galp...\"\n",
      "\n",
      "Search Warrant Pay\n",
      "\n",
      "Darius Kurmov:\n",
      "\n",
      "An Archaeologist and Sydney Esperang\n",
      "\n",
      "There is major public activity on Piltover 27 in the form of an intense wave of actions. The simultaneous hijacking of all cargo ships, smuggling and reconnaissance operations linking Piltover with Germany and Tehran, is temporary and ineffectual. Fighters from Kazortas each declared for surgical mutiny, while engineers charged with bringing care into operation appear as helpless victims of pireface without fingers and who roam through vivid oxymorons left out of dinner talk. The City's AI is activated, except by Crossing Over. Inquirer Nolf refuses to utter the words of unimaginable grief to users, now gathered in their tiny spaces, in deserted pubs, parks and malls, Rettens and shopping centres - and is convinced the (very public) kidnap of the Athena overturned the timestream: hostilities have too savage a tone to hold the last two for more than partly emotional episode padding. Complains Lycan Thontrox 25, Ekkerothious informs visitor Ron Patvan - a veteran idealist agreed to return to a blood feud due to intractable debt - that they need urgently to update their infographics. This content is running.\n",
      "\n",
      "Ecology Occult\n",
      "\n",
      "What Can We Tell About Gods From The New Hideous Origins?\n",
      "\n",
      "Ingrin Skarkov:<|endoftext|>Daylight Savings Time is a new time; for the revived Bitcoin community, perhaps the get-robot is finally emerging as a real thing that can play out between one service provider or another.\n",
      "\n",
      "A coworker of mine recently sent me a small list of three the devilish capabilities of this solar phone installation, ignoring the fact that there are many devices out there improving on fixtures like these that, due to the insufficient surface area, are difficult to analyze.\n",
      "\n",
      "Landline Cables\n",
      "\n",
      "1. Outgoing Peer Time\n",
      "\n",
      "Obviously, we are not having a heated argument over this. In fact, it's pretty much its state. People while entitled\n"
     ]
    }
   ],
   "source": [
    "# result = gpt2.generate_conditional(raw_text=\"Hello bot, tell me about yourself\")\n",
    "\n",
    "# print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Who:\n",
    "    \"\"\"A class defining the conversation parties: me, friend\"\"\"\n",
    "    def __init__(self):\n",
    "        self.prefixes = []\n",
    "\n",
    "    def matches(self,phrase):\n",
    "        for prefix in self.prefixes:\n",
    "            if phrase.startswith(prefix):\n",
    "            #print(f\"{phrase} starts with {prefix}\")\n",
    "                return True\n",
    "\n",
    "    #print(f\"{phrase} does not start with {self.prefixes}\")\n",
    "        return False\n",
    "\n",
    "    def get_random_prefix(self):\n",
    "        return self.prefixes[0] #just use first one\n",
    "\n",
    "class Friend(Who):\n",
    "    def __init__(self):\n",
    "        super().__init__()  #initializes prefixes from who\n",
    "        self.prefixes = [\"Friend said: \\\"\"]\n",
    "   \n",
    "  \n",
    "class You(Who):\n",
    "    def __init__(self):\n",
    "        super().__init__() \n",
    "        self.prefixes = [\"You said: \\\"\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Chatter:\n",
    "  \n",
    "    def __init__(self, prior = None):\n",
    "        if prior is None:\n",
    "            prior=\"\"\"\n",
    "              You: \"Hello, are you my friend?\"\n",
    "              Friend: \"Of course!\"\n",
    "              \"\"\"\n",
    "        self.suggestion = None\n",
    "\n",
    "        self.friend = Friend()\n",
    "        self.you = You()\n",
    "        self.parties  = [ self.friend, self.you ]\n",
    "\n",
    "        self.conversation = []\n",
    "\n",
    "        lines = prior.split(\"\\n\")\n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            if len(line)!=0:\n",
    "                party = None\n",
    "                for party in self.parties:\n",
    "                    if party.matches(line):\n",
    "                        break\n",
    "                if party is None:\n",
    "                    raise Exception(f\"Unknown party: {line}\")\n",
    "                self.conversation.append((party,line))\n",
    "        self.get_suggestion()\n",
    "\n",
    "\n",
    "    def get_prior(self):\n",
    "        conv = \"\"\n",
    "        for (party, line) in self.conversation:\n",
    "            conv+=line+\"\\n\"\n",
    "        return conv\n",
    "\n",
    "    def get_suggestion(self):\n",
    "        who, last_line = self.conversation[-1]\n",
    "\n",
    "        party_index = self.parties.index(who)\n",
    "        next_party = self.parties[(party_index+1) % len(self.parties)]\n",
    "\n",
    "        conv = self.get_prior()\n",
    "        conv += next_party.get_random_prefix()\n",
    "        answer = self.get_answer(next_party, conv)\n",
    "\n",
    "        if not next_party.matches(answer):\n",
    "            prefix = next_party.get_random_prefix()\n",
    "            answer = prefix + answer\n",
    "\n",
    "        self.suggestion = (next_party, answer)\n",
    "\n",
    "    def next(self, party = None, answer = \"\"):\n",
    "        \"\"\"Continue the conversation\n",
    "        :param party: None -> use the current party which is currently in turn\n",
    "        :param answer: None -> use the suggestion, specify a text to override the \n",
    "               suggestion\n",
    "\n",
    "        \"\"\"\n",
    "        suggested_party, suggested_answer = self.suggestion\n",
    "        if party is None:\n",
    "            party = suggested_party\n",
    "\n",
    "        if answer == \"\":\n",
    "            answer = suggested_answer\n",
    "\n",
    "        if not party.matches(answer):\n",
    "            prefix = party.get_random_prefix()\n",
    "            answer = prefix + answer\n",
    "\n",
    "        answer = answer.strip()\n",
    "        if answer[-1] != \"\\\"\":\n",
    "          # add the closing \"\n",
    "            answer += \"\\\"\"\n",
    "\n",
    "        self.conversation.append((party, answer))    \n",
    "        self.get_suggestion()\n",
    "\n",
    "    def retry(self):\n",
    "        self.get_suggestion()\n",
    "\n",
    "    def get_answer(self, party, conv):\n",
    "        answer = gpt2.generate_conditional(raw_text=conv)\n",
    "        lines = answer.split(\"\\n\")\n",
    "        line = \"\"\n",
    "        for line in lines:\n",
    "            if line !=\"\":\n",
    "                break\n",
    "\n",
    "\n",
    "        if line!=\"\":\n",
    "            return line\n",
    "\n",
    "        return \"\"\n",
    "\n",
    "    def show(self):\n",
    "        conv = \"\"\n",
    "        for (party, line) in self.conversation:\n",
    "            conv+=line+\"\\n\"\n",
    "        print(conv)\n",
    "        if self.suggestion is not None:\n",
    "            party, answer  = self.suggestion\n",
    "            print(\"--> \"+answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = Chatter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You: \"Hello, are you my friend?\"\n",
      "Friend: \"Of course!\"\n",
      "\n",
      "--> Friend said: \"Please trust me.\"\n"
     ]
    }
   ],
   "source": [
    "c.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "c.retry()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "c.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "c.next(c.you, \"My favorite video game is Overwatch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You: \"Hello, are you my friend?\"\n",
      "Friend: \"Of course!\"\n",
      "Friend said: \"Let's head back to the train station.\"\n",
      "You said: \"My favorite video game is Overwatch\"\n",
      "\n",
      "--> Friend said: \"Oh, not you.\"\n"
     ]
    }
   ],
   "source": [
    "c.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "c.retry()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You: \"Hello, are you my friend?\"\n",
      "Friend: \"Of course!\"\n",
      "Friend said: \"Let's head back to the train station.\"\n",
      "You said: \"My favorite video game is Overwatch\"\n",
      "You said: \"what is your favorite game?\"\n",
      "\n",
      "--> Friend said: \"rolling dice.\"\n"
     ]
    }
   ],
   "source": [
    "c.next(c.you, \"what is your favorite game?\")\n",
    "c.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "c.next(c.you, \"why do you like rolling dice?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_p36",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
